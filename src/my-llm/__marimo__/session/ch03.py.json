{
  "version": "1",
  "metadata": {
    "marimo_version": "0.18.4"
  },
  "cells": [
    {
      "id": "setup",
      "code_hash": "f80cd351b905c364088fa51d0559db39",
      "outputs": [],
      "console": []
    },
    {
      "id": "Hbol",
      "code_hash": "356c566b8e57516ed4dbea29c5388573",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h1 id=\"chapter-3-coding-attention-mechanisms\">Chapter 3: Coding Attention Mechanisms</h1></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "MJUe",
      "code_hash": "ac76a1258bd3ba7aa8436da2d786332e",
      "outputs": [],
      "console": []
    },
    {
      "id": "vblA",
      "code_hash": "1b72de0e56ac0f117e6e6df8195f0789",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"33-attending-to-different-parts-of-the-input-with-self-attention\">3.3 Attending to different parts of the input with self-attention</h2></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "bkHC",
      "code_hash": "e6d4c34d291262167f7fd630fd51f17e",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"331-a-simple-self-attention-mechanism-without-trainable-weights\">3.3.1 A simple self-attention mechanism without trainable weights</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "vEBW",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "lEQa",
      "code_hash": "47255886eacf9cf4f4a947a0778c9134",
      "outputs": [],
      "console": []
    },
    {
      "id": "UmEG",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "PKri",
      "code_hash": "a1594c97a55118afe21a78a5898d58dc",
      "outputs": [],
      "console": []
    },
    {
      "id": "woaO",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "Xref",
      "code_hash": "b90b7a557ea42d0cea739437f307bdb9",
      "outputs": [],
      "console": []
    },
    {
      "id": "kLmu",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "SFPL",
      "code_hash": "b986af97de8771c78b2b6d7f034caf64",
      "outputs": [],
      "console": []
    },
    {
      "id": "LkGn",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "BYtC",
      "code_hash": "01d426fe90a7f1e71cc40ff0f1439ba8",
      "outputs": [],
      "console": []
    },
    {
      "id": "IWgg",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "RGSE",
      "code_hash": "9b0c68b9ed02b40ee9e41a2a5f733fff",
      "outputs": [],
      "console": []
    },
    {
      "id": "HnMC",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "Kclp",
      "code_hash": "46ee78f8dcf59c377491736e1dc64755",
      "outputs": [],
      "console": []
    },
    {
      "id": "IaQp",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "emfo",
      "code_hash": "b10b78de36e39ff699f81a2954584990",
      "outputs": [],
      "console": []
    },
    {
      "id": "Hstk",
      "code_hash": "a4847ec0a2934b837c4d8285d9636c51",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"332-computing-attention-weights-for-all-input-tokens\">3.3.2 Computing attention weights for all input tokens</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "wadT",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "nWHF",
      "code_hash": "f6580dd1727c68985a22f6638e834747",
      "outputs": [],
      "console": []
    },
    {
      "id": "RKFZ",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "iLit",
      "code_hash": "a725b139ea3d9bdedddff16283b461a4",
      "outputs": [],
      "console": []
    },
    {
      "id": "IpqN",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "ZHCJ",
      "code_hash": "608f1b575147585791ed731dd36f25f3",
      "outputs": [],
      "console": []
    },
    {
      "id": "zVRe",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "ROlb",
      "code_hash": "445458740f1514ef2df8a3a72339717b",
      "outputs": [],
      "console": []
    },
    {
      "id": "TTti",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "qnkX",
      "code_hash": "a1fe05bbceaec97ebd020e2ebcdda449",
      "outputs": [],
      "console": []
    },
    {
      "id": "dxZZ",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "TqIu",
      "code_hash": "1c9880bf9457187d957fbc550117ca2f",
      "outputs": [],
      "console": []
    },
    {
      "id": "Vxnm",
      "code_hash": "b63f1b7cd636c0ab92ce784323a145c4",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"34-implementing-self-attention-with-trainable-weights\">3.4 Implementing self-attention with trainable weights</h2></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "DnEU",
      "code_hash": "c6405f6ef21ed490604fe1accb5aadb9",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"341-computing-the-attention-weights-step-by-step\">3.4.1 Computing the attention weights step by step</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "dlnW",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "ulZA",
      "code_hash": "d287737fd5b0281e7d4ba526063d9f1a",
      "outputs": [],
      "console": []
    },
    {
      "id": "fCoF",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "ecfG",
      "code_hash": "bfba957e421d854f07c7e00bb331a6ae",
      "outputs": [],
      "console": []
    },
    {
      "id": "SYQT",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "Pvdt",
      "code_hash": "8a0ab3e2f2061cb981d40528f2f95032",
      "outputs": [],
      "console": []
    },
    {
      "id": "vGiW",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "ZBYS",
      "code_hash": "7e134a8474ea63bd5d14de15a2913e13",
      "outputs": [],
      "console": []
    },
    {
      "id": "PSUk",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "aLJB",
      "code_hash": "e0ac41aaeb3878ee9e6d339ebfd03195",
      "outputs": [],
      "console": []
    },
    {
      "id": "hgqU",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "nHfw",
      "code_hash": "79c4c8a1de94c71f29e7a979d5adcebd",
      "outputs": [],
      "console": []
    },
    {
      "id": "mfOT",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "xXTn",
      "code_hash": "2fbca3f0d4f9861a1165a136de3feaf2",
      "outputs": [],
      "console": []
    },
    {
      "id": "VCRE",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "AjVT",
      "code_hash": "84d74d9186e3058175dd86fce965c7bf",
      "outputs": [],
      "console": []
    },
    {
      "id": "pHFh",
      "code_hash": "7cd1c33c5e9d4edaa4e095c5f8bc4db8",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"342-implementing-a-compact-selfattention-class\">3.4.2 Implementing a compact SelfAttention class</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "NCOB",
      "code_hash": "c5c53fe2e910150f9078f90658ba01df",
      "outputs": [],
      "console": []
    },
    {
      "id": "PSQn",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "aqbW",
      "code_hash": "22fb93a91eb5d37f74addf87800a5af5",
      "outputs": [],
      "console": []
    },
    {
      "id": "bMrW",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "TRpd",
      "code_hash": "96c60e91e0d16427007e35a4e2af54be",
      "outputs": [],
      "console": []
    },
    {
      "id": "Plbk",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "TXez",
      "code_hash": "f60c3691241f0e09c605f83b01c3cbf2",
      "outputs": [],
      "console": []
    },
    {
      "id": "dNNg",
      "code_hash": "8bf3ac6400f1ff6082dab26d161ab918",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"35-hiding-future-words-with-causal-attention\">3.5 Hiding future words with causal attention</h2></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "yCnT",
      "code_hash": "57c511ce00e971f7be359be966819ceb",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"351-applying-a-causal-attention-mask\">3.5.1 Applying a causal attention mask</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "rSYo",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "wlCL",
      "code_hash": "cd842cbe5b737cff15c08229cf06e56b",
      "outputs": [],
      "console": []
    },
    {
      "id": "HuZB",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "kqZH",
      "code_hash": "54ba0591bc55a74e73ed905c165099af",
      "outputs": [],
      "console": []
    },
    {
      "id": "WfYj",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "wAgl",
      "code_hash": "a933b55c5e91dfd6e2e18a78100fdf29",
      "outputs": [],
      "console": []
    },
    {
      "id": "LqFA",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "rEll",
      "code_hash": "5999220bcce3b0bff4ac0e1c7d141703",
      "outputs": [],
      "console": []
    },
    {
      "id": "OfTS",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "dGlV",
      "code_hash": "b6fcb9fefa5df01ebac9465b12cee6a3",
      "outputs": [],
      "console": []
    },
    {
      "id": "Ynfw",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "SdmI",
      "code_hash": "19fd6ed612097d7dd08f02b5374de1ff",
      "outputs": [],
      "console": []
    },
    {
      "id": "lgWD",
      "code_hash": "b251d41af776af72e8ff16cba385925b",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"352-masking-additional-attention-weights-with-dropout\">3.5.2 Masking additional attention weights with dropout</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "uDnK",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "yOPj",
      "code_hash": "798e5ef66eeb2410f27a6ab9d2fe7bd5",
      "outputs": [],
      "console": []
    },
    {
      "id": "lQxp",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "fwwy",
      "code_hash": "6628c0ce8df424d1a3c72cf295c6c3a8",
      "outputs": [],
      "console": []
    },
    {
      "id": "LJZf",
      "code_hash": "fc8ed30bfdf2007d29e2ee5c58f10cc0",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"353-implementing-a-compact-causal-self-attention-class\">3.5.3 Implementing a compact causal self-attention class</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "aWBL",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "urSm",
      "code_hash": "5522df6e8ba9f95859f57e6229b13e6f",
      "outputs": [],
      "console": []
    },
    {
      "id": "MIsd",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "jxvo",
      "code_hash": "c3cc0162044bcf503c1e450245f52ee8",
      "outputs": [],
      "console": []
    },
    {
      "id": "IrqS",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "mWxS",
      "code_hash": "2efd208570b939fd2c2f6108a4cee725",
      "outputs": [],
      "console": []
    },
    {
      "id": "CcZR",
      "code_hash": "6a633c1e552821a320f3a8ccde69baac",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h2 id=\"36-extending-single-head-attention-to-multi-head-attention\">3.6 Extending single-head attention to multi-head attention</h2></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "YWSi",
      "code_hash": "f743c24923cb102829cb8d88ecbbdb2c",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"361-stacking-multiple-single-head-attention-layers\">3.6.1 Stacking multiple single-head attention layers</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "Lpqv",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "zlud",
      "code_hash": "302bbb3cc5df7a190635223dca24188f",
      "outputs": [],
      "console": []
    },
    {
      "id": "CLip",
      "code_hash": "0170443c3aaa4f51bb15d71bdad37e77",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><span class=\"paragraph\">Define this class in a cell without other expressions to be imported by other notebooks.\nUse <code>@app.class_definition</code> decorator for this class to export (see this Python file itself).</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "tZnO",
      "code_hash": "850f7829fa45f895a8ad8a12f869d461",
      "outputs": [],
      "console": []
    },
    {
      "id": "xvXZ",
      "code_hash": "d47cb323919be524729487e24e60f574",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/markdown": "<span class=\"markdown prose dark:prose-invert contents\"><h3 id=\"362-implementing-multi-head-attention-with-weight-splits\">3.6.2 Implementing multi-head attention with weight splits</h3></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "upgv",
      "code_hash": null,
      "outputs": [],
      "console": []
    },
    {
      "id": "YECM",
      "code_hash": "027fa7b6295fb0c5a0d21b4adf399799",
      "outputs": [],
      "console": []
    },
    {
      "id": "cEAS",
      "code_hash": "2547ff1060a2f35548ccb196faa2f385",
      "outputs": [],
      "console": []
    },
    {
      "id": "iXej",
      "code_hash": "a8880ff9e1477af424f6fe4c00de1929",
      "outputs": [],
      "console": []
    },
    {
      "id": "EJmg",
      "code_hash": "ae4d3ffaad860d92855d7acf4f61c8ce",
      "outputs": [],
      "console": []
    }
  ]
}