<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/favicon.ico" />
    <!-- Preload is necessary because we show these images when we disconnect from the server,
    but at that point we cannot load these images from the server -->
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/gradient-yHQUC_QB.png" as="image" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/noise-60BoTA8O.png" as="image" />
    <!-- Preload the fonts -->
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/Lora-VariableFont_wght-B2ootaw-.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/PTSans-Regular-CxL0S8W7.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/PTSans-Bold-D9fedIX3.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/FiraMono-Regular-BTCkDNvf.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/FiraMono-Medium-DU3aDxX5.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/FiraMono-Bold-CLVRCuM9.ttf" as="font" crossorigin="anonymous" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta name="description" content="a marimo app" />
    <link rel="apple-touch-icon" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/apple-touch-icon.png" />
    <link rel="manifest" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/manifest.json" />

    <script data-marimo="true">
      function __resizeIframe(obj) {
        const scrollbarHeight = 20; // Max between windows, mac, and linux

        function setHeight() {
          // Guard against race condition where iframe isn't ready
          if (!obj.contentWindow?.document?.documentElement) {
            return;
          }
          const element = obj.contentWindow.document.documentElement;
          // If there is no vertical scrollbar, we don't need to resize the iframe
          if (element.scrollHeight === element.clientHeight) {
            return;
          }

          // Create a new height that includes the scrollbar height if it's visible
          const hasHorizontalScrollbar = element.scrollWidth > element.clientWidth;
          const newHeight = element.scrollHeight + (hasHorizontalScrollbar ? scrollbarHeight : 0);

          // Only update the height if it's different from the current height
          if (obj.style.height !== `${newHeight}px`) {
            obj.style.height = `${newHeight}px`;
          }
        }

        // Resize the iframe to the height of the content and bottom scrollbar height
        setHeight();

        // Resize the iframe when the content changes
        const resizeObserver = new ResizeObserver((_entries) => {
          setHeight();
        });
        // Only observe if iframe content is ready
        if (obj.contentWindow?.document?.body) {
          resizeObserver.observe(obj.contentWindow.document.body);
        }
      }
    </script>
    <marimo-filename hidden>ch03.py</marimo-filename>
    <!-- TODO(Trevor): Legacy, required by VS Code plugin. Remove when plugin is updated (see marimo/server/_templates/template.py) -->
    <marimo-version data-version="{{ version }}" hidden></marimo-version>
    <marimo-user-config data-config="{{ user_config }}" hidden></marimo-user-config>
    <marimo-server-token data-token="{{ server_token }}" hidden></marimo-server-token>
    <!-- /TODO -->
    <title>ch03</title>
    <script type="module" crossorigin crossorigin="anonymous" src="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/index-3OIHLLq_.js"></script>
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/preload-helper-g1IWzEZ-.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/chunk-LvLJmgfZ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/react-XTZdWHwa.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/compiler-runtime-QczY1yfQ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/useEventListener-4mpeqPxW.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/react-dom-BZDtXk6t.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/jsx-runtime-DUsrKwmN.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/Combination-D2HqSOvH.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/clsx-FpJB9huX.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/cn-Cs2e4A8T.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/menu-items-B57SjdWM.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/hotkeys-Dh4gy4JD.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/button-D8mnDDLX.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-Dj-O1Spr.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/createLucideIcon-BJ2I6csQ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/check-CWbw3kKW.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/select-CgZgQ5hV.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/multi-map-D3NvLiI9.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/chevron-right-B_qBg_RU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dropdown-menu-Ciqx4yT-.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/tooltip-DqNvr2Rw.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/_Uint8Array-BGESiCQL.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/_getTag-BWqNuuwU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/_baseIsEqual-C8v2yRqO.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/jotai-D74Ef_p1.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/invariant-Amh6wt4N.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/isArrayLikeObject-BkJc8lDE.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/merge-5YNOh9ZD.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/zod-BBqism5D.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/utils-x9KlwLm7.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/requests-THIDPq-F.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/once-BxRirs3K.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/toDate-Y5T6FHPx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/isSymbol-BGkTcW3U.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/toString-DlRqgfqz.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/_baseSlice-BoaxdS7L.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/_hasUnicode-CNlKPr15.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/useLifecycle-8fgb5Nue.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/capitalize-L9-XHPtm.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/ai-model-dropdown-CO52rzQt.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/badge-Cm0WUI-_.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/use-toast-DKTyV2AR.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/constants-BmIIVBf_.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/Deferred-9HYzcD3r.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/config-CbG7e0K6.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/uuid-B8w6pjWV.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/DeferredRequestRegistry-DfLQSvoS.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/connection-B11rCPnY.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/useNonce-D8CLFIrY.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/capabilities-Bdlrhekp.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/createReducer-VgoBNe3M.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-C3a_7WFa.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-0z0sEYV-.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-BzfTz_Dx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-DmruaXMM.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-DU0YrNRK.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-_bGPtAsk.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-DsSBESho.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-B2TjmP7E.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-BzDb7E9f.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/stex-C4ZxI0N3.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/cjs-Ba5CQnxe.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/_arrayReduce-DdxhySYh.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/type-kFt-n-jZ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/_baseProperty-D0ckTA0L.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/toNumber-GB_lFVaq.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/now-CAWn5i9V.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/debounce-Bgq-wc7M.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/toInteger-Cl1doThW.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/main-CC_mbgxK.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/cells-DoKM12vd.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/kbd-EdFdwHEs.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/renderShortcut-CLenKJO6.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/useEvent-l9BzRqpK.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/useDeleteCell-BDBSmjLV.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/lazy-Cbg8zRvU.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/defaultLocale-8c1ixjJ_.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/precisionRound-CQhbsDNz.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/defaultLocale-BECdiY5x.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/vega-loader.browser-DZ31-605.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/tooltip-CSAGVmjx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/ErrorBoundary-6EjkHrSV.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/alert-dialog-D9SwG8oA.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dialog-BPIq0Dmw.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/useDebounce-C9S6xmGh.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/numbers-BigybruE.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/SSRProvider-DlZbKHsu.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/context-CO64Gasq.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/useNumberFormatter-B5HAJDUq.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/usePress-DHxqjtDm.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/input-CVoLDDk-.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/ImperativeModal-ameyIrhc.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/cell-link-BiAu_8pk.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/alert-D5BMn3Et.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/links-Iv0WhyKW.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/state-i7G4J1VS.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/MarimoErrorOutput-Bh7Bgl2Z.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/copy-BS3uwoaq.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/copy-DhmkXAPp.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/copy-icon-Lyk2_Z8G.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/spinner-DpaQpaiI.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-D1pae9A2.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/focus-CyxSol0W.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/popover-DZBXSGEz.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/useRunCells-DT1En-Te.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/mode-DKDAYLK9.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/purify.es-hXMlfS4W.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/RenderHTML-Dlz0Ew2i.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/table-CesNYRF-.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/tabs-BjAbB8w_.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/add-missing-import-DV1FOw9w.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/useAsyncData-_-DLduEH.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/useIframeCapabilities-r0LQMxAQ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/LazyAnyLanguageCodeMirror-C7KtNlRp.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/error-banner-C9tkg1MX.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/formats-C3exma5i.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/useTheme-DpA-8Qyy.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/en-US-Lc4gASCJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/isValid-Cd_L_2Pm.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/maps-D4abje2p.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/shim-D-ElLTBO.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/download-CCvBQ7qL.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/extends-C2lJEiP_.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/emotion-is-prop-valid.esm-c4A1huxA.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/useDateFormatter-BMdQ9MI_.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/katex-BmfUoCWW.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/marked.esm-idL59iHw.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/chunk-T7OEMACY-CdGEwedE.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/range-Bbli-TEL.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/table-kHJ0xpFt.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/Output-DxdWGPFi.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/name-cell-input-C6ZYU6mN.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/useHotkey-B0ylhc2m.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-CqHWOZu2.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-BcLYYUSi.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-BVXr6b5T.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-BrEX5EQl.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-CalRK9ms.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-Cc4T7UUX.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-D3MC5WiF.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-DgaCIZAa.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-rqHtY98i.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-tj-jaP-y.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-w7c0BKrD.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-CCZcRJI9.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-CrDb6akR.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/apl-BQt6lLuE.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/asciiarmor-BjX7bs0j.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/asn1-xn1MtyoW.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/brainfuck-BjO-UY1i.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/clike-DW9tQWfF.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/clojure-BSW96HRx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/cmake-C7UlMfz3.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/cobol-CyWIev3Y.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/coffeescript-DTvziYTK.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/commonlisp-yMdkdGMi.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/crystal-BGnoSp3z.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/css-D7fWzevp.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/cypher-Cr1dl0Qf.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/d-C2bS5bA6.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/diff-B2mHQyn6.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dtd-Cy-lUE7p.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dylan-mm60grwk.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/ecl-Cqi05Wy1.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/eiffel-FkvfLQq7.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/elm-Da5txHw0.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/erlang-CdnCEH5x.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/simple-mode-B185hCuE.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/factor-CamlR8EJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/forth-UcMX0m31.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/fortran-DZ_vNVsP.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/gas-CM88o6Zj.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/gherkin-BhrpTeN9.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/groovy-BFju68Wx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/haskell-CrWHY4et.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/haxe-1eKW-tbO.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/idl-Do4AwvU2.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/javascript-CTXZKRYd.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/julia-BRynwHI4.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/livescript-ZmIBNA-I.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/lua-mUfZ-MRC.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/mathematica-Dil5dTHO.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/mbox-Dnkn9wSr.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/mirc-auEWieOB.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/mllike-xVJsauJ8.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/modelica-B8kajVDm.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/mscgen-Bsnta1jn.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/mumps-CMw16b5q.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/nsis-DwPdi8sw.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/ntriples-O5lQxirn.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/octave-CMfAar-E.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/oz-CLRI9Qww.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/pascal-D2M_3HBx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/perl-B8lV7aKt.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/pig-Bima2bfE.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/powershell-v_t41ydu.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/properties-ChrStSby.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/protobuf-Bin6kmlf.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/pug-UWVdYLYW.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/puppet-DYrtWvYn.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/python-BuKrulde.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/q-CrXUTiVN.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/r-Bk2Tiso2.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/rpm-CmmZphz3.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/ruby-ElyOTVeQ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/sas-jE3ZlG0K.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/scheme-CZF1tRbv.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/shell-LqT3ZqMa.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/sieve-y15yT7Pp.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/smalltalk-pk5PvsMs.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/sparql-DTZ_gfPQ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/stylus-CrGj0eNT.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/swift-tt3PlBbI.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/tcl-1P4b0GMJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/textile-BHDIuMYh.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/toml-C8qoqk_i.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/troff-DMiF5Zmz.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/ttcn-cfg-qLZtNU-E.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/ttcn-BAUp3RZn.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/turtle-BizIwmKv.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/vb-Bn_uCxNL.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/vbscript-BTKW8Atx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/velocity-CcB05fpi.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/verilog-BWdtOJH8.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/vhdl-CzFOaOHx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/webidl-DDEKCqSy.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/xquery-BcF6ZhQP.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/yacas-BARQAiXa.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/z80-DO_Ep0iT.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/esm-DnNh7vdQ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/objectWithoutPropertiesLoose-DDabolgJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/esm-Crqxhl3F.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/trash-Df8Wb7xS.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/ConsoleOutput-D8ETq-cc.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/icons-BgmgMjOV.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/blob-LiDbeYko.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/add-cell-with-ai-85tnyxYo.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/chart-no-axes-column-BrQ3B3xc.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/square-function-D5EEjILG.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/spec-DpllSlJD.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/useInstallPackage-BaAhpZlP.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/column-preview-DGBk0umL.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/useAddCell-DYfGo4T8.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/toggle-PGHtwx2G.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/api-CoLss8_O.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/globals-oV9pRUSB.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/share-DjbnD3WE.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/memoize-DN0TMY36.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/get-CyLJYAfP.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/_baseSet-CKSmZC_I.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/utilities.esm-DvrcU9Ky.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/floating-outline-DmCT2Mvg.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/eye-off-BUwXWtH0.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/plus-CiGrWbgb.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/readonly-python-code-CkaJ4Tks.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/file-video-camera-BGBfFNDn.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/file-text-O5IflCIn.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/file-CqXxt3_J.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/types-BMriRmf-.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/label-4Nancvon.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/textarea-BnZHy5Jt.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/circle-x-Cc9X6U_B.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/refresh-ccw-aU2rUkrI.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/trash-2-Du5ITnDv.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/form-600t_Civ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/field-Dov6YyOV.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/switch-Ct8mUApv.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/useBoolean-Dmwks1uh.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/useDeepCompareMemoize-C8RdAYFq.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/types-D0WVpW93.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/prop-types-WJwGTMoZ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/es-DAmFxw3U.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/VisuallyHidden-RqykLBKh.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/_baseFlatten-rM2FaX4x.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/hasIn-CE-pxQP-.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/_basePickBy-LbasZ8ix.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/pick-FbHfDtcJ.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/send-LAFRRDPT.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/code-xml-DdSOsi73.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/download-Dmmu2U4a.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/globe-vMZtva7z.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/refresh-cw-BOc7UXm9.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/settings-DznrHqbd.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/square-DhlBDqyx.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/triangle-alert-Bf1kg4xT.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/dist-DL3W3q-2.js">
    <link rel="modulepreload" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/bundle.esm-C-9IAWQU.js">
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/cells-wU8qigWB.css">
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/Output-DUep6sC9.css">
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/ConsoleOutput-BFFTcC5Y.css">
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.18.4/dist/assets/index-CJU5NUvF.css">
  
<script data-marimo="true">
    window.__MARIMO_STATIC__ = {};
    window.__MARIMO_STATIC__.files = {};
</script>
</head>
  <body>
    <div id="root"></div>
    <!-- This is a portal for the data editor to render in -->
    <div id="portal" data-testid="glide-portal" style="position: fixed; left: 0; top: 0; z-index: 9999"></div>
    <script data-marimo="true">
      window.__MARIMO_MOUNT_CONFIG__ = {
            "filename": "ch03.py",
            "mode": "read",
            "version": "0.18.4",
            "serverToken": "static",
            "config": {"ai": {"models": {"custom_models": [], "displayed_models": []}}, "completion": {"activate_on_typing": true, "copilot": false}, "diagnostics": {"sql_linter": true}, "display": {"cell_output": "below", "code_editor_font_size": 14, "dataframes": "rich", "default_table_max_columns": 50, "default_table_page_size": 10, "default_width": "medium", "reference_highlighting": false, "theme": "light"}, "formatting": {"line_length": 79}, "keymap": {"overrides": {}, "preset": "default"}, "language_servers": {"pylsp": {"enable_flake8": false, "enable_mypy": true, "enable_pydocstyle": false, "enable_pyflakes": false, "enable_pylint": false, "enable_ruff": true, "enabled": false}}, "mcp": {"mcpServers": {}, "presets": []}, "package_management": {"manager": "uv"}, "runtime": {"auto_instantiate": false, "auto_reload": "off", "default_sql_output": "auto", "on_cell_change": "autorun", "output_max_bytes": 8000000, "reactive_tests": true, "std_stream_max_bytes": 1000000, "watcher_on_save": "lazy"}, "save": {"autosave": "after_delay", "autosave_delay": 1000, "format_on_save": false}, "server": {"browser": "default", "follow_symlink": false}, "snippets": {"custom_paths": [], "include_default_snippets": true}},
            "configOverrides": {},
            "appConfig": {"sql_output": "auto", "width": "medium"},
            "view": {"showAppCode": true},
            "notebook": {"cells": [{"code": "from importlib.metadata import version\n\nimport marimo as mo\nimport torch\nimport torch.nn as nn", "code_hash": "f80cd351b905c364088fa51d0559db39", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "setup", "name": "setup"}, {"code": "mo.md(r\"\"\"\n# Chapter 3: Coding Attention Mechanisms\n\"\"\")", "code_hash": "356c566b8e57516ed4dbea29c5388573", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Hbol", "name": "_"}, {"code": "print(\"torch version:\", version(\"torch\"))", "code_hash": "ac76a1258bd3ba7aa8436da2d786332e", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "MJUe", "name": "_"}, {"code": "mo.md(r\"\"\"\n## 3.3 Attending to different parts of the input with self-attention\n\"\"\")", "code_hash": "1b72de0e56ac0f117e6e6df8195f0789", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "vblA", "name": "_"}, {"code": "mo.md(r\"\"\"\n### 3.3.1 A simple self-attention mechanism without trainable weights\n\"\"\")", "code_hash": "e6d4c34d291262167f7fd630fd51f17e", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "bkHC", "name": "_"}, {"code": "mo.md(r\"\"\"\nPrepare embedded vectors as inputs to attention layers.\n\"\"\")", "code_hash": "4ecdd60550fc95574f958fec3932c248", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "lEQa", "name": "_"}, {"code": "inputs = torch.tensor(\n    [\n        [0.43, 0.15, 0.89],  # Your     (x^1)\n        [0.55, 0.87, 0.66],  # journey  (x^2)\n        [0.57, 0.85, 0.64],  # starts   (x^3)\n        [0.22, 0.58, 0.33],  # with     (x^4)\n        [0.77, 0.25, 0.10],  # one      (x^5)\n        [0.05, 0.80, 0.55],  # step     (x^6)\n    ]\n)", "code_hash": "47255886eacf9cf4f4a947a0778c9134", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "PKri", "name": "_"}, {"code": "mo.md(r\"\"\"\nAttenstion score $\\omega_{2i}$ is defined as\n$$\n\\vec{\\omega}_{2i} \\equiv \\vec{x}_2 \\cdot \\vec{x}_i.\n$$\n\"\"\")", "code_hash": "a0c210f2a0721e6d4c4b53078999e77f", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Xref", "name": "_"}, {"code": "query = inputs[1]  # 2nd input token is the query\n\nattn_scores_2_trainless = torch.empty(inputs.shape[0])\nfor _i, _x_i in enumerate(inputs):\n    # dot product (transpose not necessary here since they are 1-dim vectors)\n    attn_scores_2_trainless[_i] = torch.dot(_x_i, query)\nprint(attn_scores_2_trainless)", "code_hash": "577bdd9505b0d5e67c0217711991718b", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "SFPL", "name": "_"}, {"code": "mo.md(r\"\"\"\nThis is a verification of dot product for beginners by\n$$\n\\vec{x}_0\\cdot\\vec{x}_1\\equiv\\sum_{i=0}^{\\mathrm{dim}(\\vec{x}_0)}(x_0)_i(x_1)_i.\n$$\nThe dot product has meaning of similarity between two vectors.\n\"\"\")", "code_hash": "8cc4d5403ad6581e162fe069f8a90c29", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "BYtC", "name": "_"}, {"code": "_res = 0.0\nfor _idx, _element in enumerate(inputs[0]):\n    _res += inputs[0][_idx] * query[_idx]\n\nprint(_res)\nprint(torch.dot(inputs[0], query))", "code_hash": "b90b7a557ea42d0cea739437f307bdb9", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "RGSE", "name": "_"}, {"code": "mo.md(r\"\"\"\nThe attention should be normalized to represents weights.\n\"\"\")", "code_hash": "bef957f8fa5ba9de673528083ce46430", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Kclp", "name": "_"}, {"code": "_attn_weights_2_tmp = attn_scores_2_trainless / attn_scores_2_trainless.sum()\n\nprint(\"Attention weights:\", _attn_weights_2_tmp)\nprint(\"Sum:\", _attn_weights_2_tmp.sum())", "code_hash": "b986af97de8771c78b2b6d7f034caf64", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "emfo", "name": "_"}, {"code": "mo.md(r\"\"\"\nMore suitable function for normalization is the softmax function that has positive values and robust gradient for traning\n$$\n\\sigma(x_i;\\vec{x})\\equiv\\frac{e^{x_i}}{\\sum_j e^{x_j}}.\n$$\n\"\"\")", "code_hash": "15e9e2fffafd9fbec97dc15aff923e36", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Hstk", "name": "_"}, {"code": "def softmax_naive(x):\n    return torch.exp(x) / torch.exp(x).sum(dim=0)", "code_hash": "01d426fe90a7f1e71cc40ff0f1439ba8", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "nWHF", "name": "*softmax_naive"}, {"code": "mo.md(r\"\"\"\nThe softmax function also leads weights whose total is 1.\n\"\"\")", "code_hash": "c8f5a9fac3ab502b245a1c4f3008d817", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "iLit", "name": "_"}, {"code": "_attn_weights_2_naive = softmax_naive(attn_scores_2_trainless)\n\nprint(\"Attention weights:\", _attn_weights_2_naive)\nprint(\"Sum:\", _attn_weights_2_naive.sum())", "code_hash": "9b0c68b9ed02b40ee9e41a2a5f733fff", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "ZHCJ", "name": "_"}, {"code": "mo.md(r\"\"\"\nPyTorch softmax function is more stable with respect to numerical errors because it uses [max-trick and LogSumExp trick](https://discuss.pytorch.org/t/justification-for-logsoftmax-being-better-than-log-softmax/140130/3).\n\"\"\")", "code_hash": "fd4c9039872fafbdd63653ce5fe2325f", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "ROlb", "name": "_"}, {"code": "attn_weights_2_trainless = torch.softmax(attn_scores_2_trainless, dim=0)\n\nprint(\"Attention weights:\", attn_weights_2_trainless)\nprint(\"Sum:\", attn_weights_2_trainless.sum())", "code_hash": "46ee78f8dcf59c377491736e1dc64755", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "qnkX", "name": "_"}, {"code": "mo.md(r\"\"\"\nThe attention scores are used as weights for weighted average of embedded vectors.\nThe result is called as context vectors $\\vec{z}^{(i)}$.\n\nFormaly, the context vectors $\\vec{z}^{(i)}$ are defined as\n$$\n\\vec{z}^{(i)}\n\\equiv\\sum_j\\mathrm{Softmax}_j(\\omega_{ij})\\vec{x}_j\n\\equiv\\frac{\\sum_j\\omega_{ij}\\vec{x}_j}{\\sum_k\\omega_{ik}}\n\\equiv\\frac{(\\sum_j\\vec{x}_j\\vec{x}_j^T)\\vec{x}_i}{\\sum_k\\vec{x}_k\\cdot\\vec{x}_i}\n$$\n\"\"\")", "code_hash": "4a1e95210a8d2650d158b2663e404299", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "TqIu", "name": "_"}, {"code": "context_vec_2_trainless = torch.zeros(query.shape)\nfor _i, _x_i in enumerate(inputs):\n    context_vec_2_trainless += attn_weights_2_trainless[_i] * _x_i\n\nprint(context_vec_2_trainless)", "code_hash": "b10b78de36e39ff699f81a2954584990", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Vxnm", "name": "_"}, {"code": "mo.md(r\"\"\"\n### 3.3.2 Computing attention weights for all input tokens\n\"\"\")", "code_hash": "a4847ec0a2934b837c4d8285d9636c51", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "DnEU", "name": "_"}, {"code": "mo.md(r\"\"\"\nCalculate context vectors with respect to all query vectors.\n\"\"\")", "code_hash": "7230e61f3e5b41e543c00e367b2d5838", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "ulZA", "name": "_"}, {"code": "_attn_scores = torch.empty(6, 6)\n\nfor _i, _x_i in enumerate(inputs):\n    for _j, _x_j in enumerate(inputs):\n        _attn_scores[_i, _j] = torch.dot(_x_i, _x_j)\nprint(_attn_scores)", "code_hash": "f6580dd1727c68985a22f6638e834747", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "ecfG", "name": "_"}, {"code": "mo.md(r\"\"\"\nIn Python, such double for-loops are inefficient. The following matrix product is preferred for computation efficiency. This result is the same with the result of the previous cell.\n\"\"\")", "code_hash": "a607bd81dc431c54bcedb110e5407a15", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Pvdt", "name": "_"}, {"code": "attn_scores = inputs @ inputs.T\nprint(attn_scores)", "code_hash": "a725b139ea3d9bdedddff16283b461a4", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "ZBYS", "name": "_"}, {"code": "mo.md(r\"\"\"\nWe can apply softmax function to each rows.\n\"\"\")", "code_hash": "8815bafad990234ae36eec3e5c1dc746", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "aLJB", "name": "_"}, {"code": "attn_weights = torch.softmax(attn_scores, dim=-1)\nprint(attn_weights)", "code_hash": "608f1b575147585791ed731dd36f25f3", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "nHfw", "name": "_"}, {"code": "mo.md(r\"\"\"\nThe normalization is verified by this.\n\"\"\")", "code_hash": "32daaa92469d6bf75d5eb6b77a77751f", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "xXTn", "name": "_"}, {"code": "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nprint(\"Row 2 sum:\", row_2_sum)\n\nprint(\"All row sums:\", attn_weights.sum(dim=-1))", "code_hash": "445458740f1514ef2df8a3a72339717b", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "AjVT", "name": "_"}, {"code": "mo.md(r\"\"\"\nWeighted averages are also simplified by using matrix-vector products.\nFinally, we obtain the following formula:\n$$\nZ\\equiv\\mathrm{Softmax}(XX^T)X\n$$\n\"\"\")", "code_hash": "554c86000db51fe8d8c440887f0e6847", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "pHFh", "name": "_"}, {"code": "all_context_vecs = attn_weights @ inputs\nprint(all_context_vecs)", "code_hash": "a1fe05bbceaec97ebd020e2ebcdda449", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "NCOB", "name": "_"}, {"code": "mo.md(r\"\"\"\nThe 2nd slice is the exactly same with the result of the previous section.\n\"\"\")", "code_hash": "b474df4ce89eec717e1844456c3fef54", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "aqbW", "name": "_"}, {"code": "print(\"Previous 2nd context vector:\", context_vec_2_trainless)", "code_hash": "1c9880bf9457187d957fbc550117ca2f", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "TRpd", "name": "_"}, {"code": "mo.md(r\"\"\"\n## 3.4 Implementing self-attention with trainable weights\n\"\"\")", "code_hash": "b63f1b7cd636c0ab92ce784323a145c4", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "TXez", "name": "_"}, {"code": "mo.md(r\"\"\"\n### 3.4.1 Computing the attention weights step by step\n\"\"\")", "code_hash": "c6405f6ef21ed490604fe1accb5aadb9", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "dNNg", "name": "_"}, {"code": "mo.md(r\"\"\"\nUse these inputs and parameters to show how to introduce trainable attention layer.\n\"\"\")", "code_hash": "b221b970b84206a7e7d0f3720392b136", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "yCnT", "name": "_"}, {"code": "x_2 = inputs[1]  # second input element\nd_in = inputs.shape[1]  # the input embedding size, d=3\nd_out = 2  # the output embedding size, d=2", "code_hash": "d287737fd5b0281e7d4ba526063d9f1a", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "wlCL", "name": "_"}, {"code": "mo.md(r\"\"\"\nThese are trainable weight matricies (or linear projections).\n\"\"\")", "code_hash": "64582f38215fdaef16ce338fd647579b", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "kqZH", "name": "_"}, {"code": "torch.manual_seed(123)\n\n# requires_grad=False is for simplification purposes only (True is required for training)\nW_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)", "code_hash": "2cfd47f0efcc4ebf47ed1304b236a81a", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "wAgl", "name": "_"}, {"code": "mo.md(r\"\"\"\nThe linear projection are like these\n$$\nQ_2\\equiv \\vec{x}_2 W^Q ,\\quad\nK_2\\equiv \\vec{x}_2 W^K,\\quad\nV_2\\equiv \\vec{x}_2 W^V,\n$$\n\"\"\")", "code_hash": "faddb0748ea709832a0c599e42044372", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "rEll", "name": "_"}, {"code": "query_2 = x_2 @ W_query  # _2 because it's with respect to the 2nd input element\nkey_2 = x_2 @ W_key\nvalue_2 = x_2 @ W_value\n\nprint(query_2)", "code_hash": "8a0ab3e2f2061cb981d40528f2f95032", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "dGlV", "name": "_"}, {"code": "mo.md(r\"\"\"\nLinear projection for all inputs are\n$$\nK=XW^K, V=XW^V.\n$$\n\"\"\")", "code_hash": "ce116d1a8afeb02038a2ef38dab7fb66", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "SdmI", "name": "_"}, {"code": "keys = inputs @ W_key\nvalues = inputs @ W_value\n\nprint(\"keys.shape:\", keys.shape)\nprint(\"values.shape:\", values.shape)", "code_hash": "7e134a8474ea63bd5d14de15a2913e13", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "lgWD", "name": "_"}, {"code": "mo.md(r\"\"\"\nThe attention score $\\omega_{22}\\equiv Q_2\\cdot K_2$ is this.\n\"\"\")", "code_hash": "8425abd660941ebfbcf4854c753a222e", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "yOPj", "name": "_"}, {"code": "keys_2 = keys[1]  # Python starts index at 0\nattn_score_22 = query_2.dot(keys_2)\nprint(attn_score_22)", "code_hash": "e0ac41aaeb3878ee9e6d339ebfd03195", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "fwwy", "name": "_"}, {"code": "mo.md(r\"\"\"\nScores for all keys are calculated by $\\omega_2\\equiv Q_2 K$.\n\"\"\")", "code_hash": "6a99a6019b551355313b8454ca548f8e", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "LJZf", "name": "_"}, {"code": "attn_scores_2 = query_2 @ keys.T  # All attention scores for given query\nprint(attn_scores_2)", "code_hash": "79c4c8a1de94c71f29e7a979d5adcebd", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "urSm", "name": "_"}, {"code": "mo.md(r\"\"\"\nTake the attention scores by Scaled Dot-Product Attention way.\nThe attention score is scaled by square root of the dimension of keys like\n$$\n\\alpha_2\\equiv\\mathrm{Softmax}\\frac{\\omega_2}{\\sqrt{\\mathrm{dim}(K_i)}}\n$$\n\"\"\")", "code_hash": "2b230a782e39bd6d198910008becbca7", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "jxvo", "name": "_"}, {"code": "d_k = keys.shape[1]\nattn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\nprint(attn_weights_2)", "code_hash": "2fbca3f0d4f9861a1165a136de3feaf2", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "mWxS", "name": "_"}, {"code": "mo.md(r\"\"\"\nFinally, we get trainable context vector as\n$$\n\\vec{z}^2\\equiv\\mathrm{Softmax}\\left(\\frac{Q_2K}{\\sqrt{\\mathrm{dim}(K_i)}}\\right)V.\n$$\n\"\"\")", "code_hash": "28021fa9d228d4bc2dcb9f772c1154e7", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "CcZR", "name": "_"}, {"code": "context_vec_2 = attn_weights_2 @ values\nprint(context_vec_2)", "code_hash": "84d74d9186e3058175dd86fce965c7bf", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "YWSi", "name": "_"}, {"code": "mo.md(r\"\"\"\n### 3.4.2 Implementing a compact SelfAttention class\n\"\"\")", "code_hash": "7cd1c33c5e9d4edaa4e095c5f8bc4db8", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "zlud", "name": "_"}, {"code": "class SelfAttention_v1(nn.Module):\n    def __init__(self, d_in, d_out):\n        super().__init__()\n        # trainable parameters are stored as member variables\n        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n\n    def forward(self, x):\n        keys = x @ self.W_key\n        queries = x @ self.W_query\n        values = x @ self.W_value\n\n        attn_scores = queries @ keys.T  # omega\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n\n        context_vec = attn_weights @ values\n        return context_vec", "code_hash": "fd813f7a49bc8a40cbcd578baa9374c2", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "tZnO", "name": "*SelfAttention_v1"}, {"code": "mo.md(r\"\"\"\nThe trainable context vectors are calculated by this instance. The 2nd slice of the result is the same with the result of the previous section.\n\"\"\")", "code_hash": "71a30beabb5fb21930dfc17c9daeeef3", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "xvXZ", "name": "_"}, {"code": "torch.manual_seed(123)\nsa_v1 = SelfAttention_v1(d_in, d_out)\nprint(sa_v1(inputs))", "code_hash": "22fb93a91eb5d37f74addf87800a5af5", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "CLip", "name": "_"}, {"code": "mo.md(r\"\"\"\nBy using `nn.Linear`, the class definition is more simplified and more stable because of a good initialization scheme called as [the Kaiming Uniform initialization](https://docs.pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_).\n\"\"\")", "code_hash": "841d804357b2a390dac971bb3280646b", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "YECM", "name": "_"}, {"code": "class SelfAttention_v2(nn.Module):\n    def __init__(self, d_in, d_out, qkv_bias=False):\n        super().__init__()\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n\n    def forward(self, x):\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        attn_scores = queries @ keys.T\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n\n        context_vec = attn_weights @ values\n        return context_vec", "code_hash": "96c60e91e0d16427007e35a4e2af54be", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "cEAS", "name": "*SelfAttention_v2"}, {"code": "mo.md(r\"\"\"\nThis output is different with the previous results because of such initialization schemes.\n\"\"\")", "code_hash": "9f88dca8b8156f32cc4925bcac5e18f1", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "iXej", "name": "_"}, {"code": "torch.manual_seed(789)\nsa_v2 = SelfAttention_v2(d_in, d_out)\nprint(sa_v2(inputs))", "code_hash": "f60c3691241f0e09c605f83b01c3cbf2", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "EJmg", "name": "_"}, {"code": "mo.md(r\"\"\"\n## 3.5 Hiding future words with causal attention\n\"\"\")", "code_hash": "8bf3ac6400f1ff6082dab26d161ab918", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "UmEG", "name": "_"}, {"code": "mo.md(r\"\"\"\n### 3.5.1 Applying a causal attention mask\n\"\"\")", "code_hash": "57c511ce00e971f7be359be966819ceb", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "vEBW", "name": "_"}, {"code": "mo.md(r\"\"\"\nThis is an initial attention weights like previous.\n\"\"\")", "code_hash": "aa7e4b02aa0288ed425038ca02e54c17", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "kLmu", "name": "_"}, {"code": "# Reuse the query and key weight matrices of the\n# SelfAttention_v2 object from the previous section for convenience\n_queries = sa_v2.W_query(inputs)\n_keys = sa_v2.W_key(inputs)\n_attn_scores = _queries @ _keys.T\n\nattn_weights_sa_v2 = torch.softmax(_attn_scores / _keys.shape[-1] ** 0.5, dim=-1)\nprint(attn_weights_sa_v2)", "code_hash": "cd842cbe5b737cff15c08229cf06e56b", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "IpqN", "name": "_"}, {"code": "mo.md(r\"\"\"\nLet's mask the upper right of it to ensure causal token prediction tasks.\nSuch binary mask is easily created by `torch.tril`.\n\"\"\")", "code_hash": "afc1416a20a22e1bbf523a288e02b85c", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "dxZZ", "name": "_"}, {"code": "_context_length = attn_scores.shape[0]\nmask_simple = torch.tril(torch.ones(_context_length, _context_length))\nprint(mask_simple)", "code_hash": "54ba0591bc55a74e73ed905c165099af", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "dlnW", "name": "_"}, {"code": "mo.md(r\"\"\"\nThe masking is simple multiplication of these $W_a\\mathbb{1}_{i\\geq j}$.\n\"\"\")", "code_hash": "444d50edf93def25bb0fb23ddd306f33", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "TTti", "name": "_"}, {"code": "masked_simple = attn_weights * mask_simple\nprint(masked_simple)", "code_hash": "a933b55c5e91dfd6e2e18a78100fdf29", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "RKFZ", "name": "_"}, {"code": "mo.md(r\"\"\"\nNormalize each rows to use it as weights. The normalization should be done after the masking to prevent data leak from futures.\n\"\"\")", "code_hash": "4955678db132bc3de765fb3feaa099e7", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "IaQp", "name": "_"}, {"code": "row_sums = masked_simple.sum(dim=-1, keepdim=True)\nmasked_simple_norm = masked_simple / row_sums\nprint(masked_simple_norm)", "code_hash": "5999220bcce3b0bff4ac0e1c7d141703", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "IWgg", "name": "_"}, {"code": "mo.md(r\"\"\"\nFor more efficiency, this masking is replaced by filling `-inf` to upper right of attention scores before taking Softmax.\n\"\"\")", "code_hash": "3ab7ecedefc7269da93d6e3beaf4205c", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "fCoF", "name": "_"}, {"code": "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\nmasked = attn_scores.masked_fill(mask.bool(), -torch.inf)\nprint(masked)", "code_hash": "b6fcb9fefa5df01ebac9465b12cee6a3", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "LkGn", "name": "_"}, {"code": "mo.md(r\"\"\"\n$\\exp(-\\infty)=0$ leads the same result with the masking.\n\"\"\")", "code_hash": "e3421f9caf20181d4d21929c376b0742", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "zVRe", "name": "_"}, {"code": "causal_attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=-1)\nprint(causal_attn_weights)", "code_hash": "5d3313b0e89b9adfa164412dc0ad3f55", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "woaO", "name": "_"}, {"code": "mo.md(r\"\"\"\n### 3.5.2 Masking additional attention weights with dropout\n\"\"\")", "code_hash": "b251d41af776af72e8ff16cba385925b", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "HnMC", "name": "_"}, {"code": "mo.md(r\"\"\"\nWe introduct dropout to prevent overfitting to specific tokens. We drop 50% of elements and the remaining values are doubled to keep the total scale. This dropout layer is applied only for training.\n\"\"\")", "code_hash": "5acd40cccb5ab0b77ce60820056c8db1", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "wadT", "name": "_"}, {"code": "torch.manual_seed(123)\ndropout = torch.nn.Dropout(0.5)  # dropout rate of 50%\nexample = torch.ones(6, 6)  # create a matrix of ones\n\nprint(dropout(example))", "code_hash": "a52ceb168174043b85cac19d2261b2c6", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "VCRE", "name": "_"}, {"code": "mo.md(r\"\"\"\nThe dropout layer is applied after attention weights are calculated.\n\"\"\")", "code_hash": "2e5d4b307962790c9bb8cac1778ce7ab", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "hgqU", "name": "_"}, {"code": "torch.manual_seed(123)\nprint(dropout(attn_weights))", "code_hash": "6628c0ce8df424d1a3c72cf295c6c3a8", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "PSUk", "name": "_"}, {"code": "mo.md(r\"\"\"\n### 3.5.3 Implementing a compact causal self-attention class\n\"\"\")", "code_hash": "fc8ed30bfdf2007d29e2ee5c58f10cc0", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "mfOT", "name": "_"}, {"code": "mo.md(r\"\"\"\nWe introduce such causal attention, dropout, and batch inference to attention layer. The batch input for testing is as follows.\n\"\"\")", "code_hash": "370c21f9e8ac28cf72329e5926808ab7", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "vGiW", "name": "_"}, {"code": "batch = torch.stack((inputs, inputs), dim=0)\n# 2 inputs with 6 tokens each, and each token has embedding dimension 3\nprint(batch.shape)  ", "code_hash": "c247a774e29f9f681cfbd37259eb5810", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "SYQT", "name": "_"}, {"code": "mo.md(r\"\"\"\nThis is an example for causal attention layer. [`register_buffer`](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer) is used to define constant (non-trained) parameters and ensure it is allocated on suitable device.\n\"\"\")", "code_hash": "451d1c657edd1be831d2e531ffb1845b", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "bMrW", "name": "_"}, {"code": "class CausalAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n\n        # NEW\n        self.dropout = nn.Dropout(dropout)\n        # NEW: allocate maximum mask to be sliced later\n        self.register_buffer(\n            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n        )\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape  # New batch dimension b\n\n        # For inputs where `num_tokens` exceeds `context_length`, this will result in errors\n        # in the mask creation further below.\n        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs\n        # do not exceed `context_length` before reaching this forward method.\n        keys = self.W_key(x)    # (B,N,d_out)\n        queries = self.W_query(x)   # (B,N,d_out)\n        values = self.W_value(x)    # (B,N,d_out)\n\n        # Changed transpose: (B,N,d_out) -\u003E (B,d_out,N)\n        attn_scores = queries @ keys.transpose(1, 2)    # (B,N,N)\n        # New, _ ops are in-place\n        # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n        attn_scores.masked_fill_(  \n            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n        )  \n        # normalization\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)  # New\n\n        context_vec = attn_weights @ values # (B,N,d_out)\n        return context_vec", "code_hash": "2fb96fa912dd5f93e4408015564d621f", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "PSQn", "name": "*CausalAttention"}, {"code": "mo.md(r\"\"\"\nThis is an example to use the forward path. Check the input and output shapes.\n\"\"\")", "code_hash": "cf48c59b8820e6b4ac72bee6f5575f52", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "OfTS", "name": "_"}, {"code": "torch.manual_seed(123)\n\n# `batch` has (B,N,C) shape\n_context_length = batch.shape[1]\nca = CausalAttention(d_in, d_out, _context_length, 0.0)\n\n_context_vecs = ca(batch)\n\nprint(_context_vecs)\n\nprint(f\"{batch.shape=}\")\nprint(f\"{_context_vecs.shape=}\")\nprint(f\"{d_in=}, {d_out=}\")", "code_hash": "e7b136ad731c5decb153b4251a64af94", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "lQxp", "name": "_"}, {"code": "mo.md(r\"\"\"\n## 3.6 Extending single-head attention to multi-head attention\n\"\"\")", "code_hash": "6a633c1e552821a320f3a8ccde69baac", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Plbk", "name": "_"}, {"code": "mo.md(r\"\"\"\n### 3.6.1 Stacking multiple single-head attention layers\n\"\"\")", "code_hash": "f743c24923cb102829cb8d88ecbbdb2c", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "rSYo", "name": "_"}, {"code": "mo.md(r\"\"\"\nLet's extend the attention layer to multi-head. This is a naive extetion from single-head one.\n\"\"\")", "code_hash": "bf56d8bf893de06c4814455909aff84b", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "HuZB", "name": "_"}, {"code": "class MultiHeadAttentionWrapper(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        self.heads = nn.ModuleList(\n            [\n                CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n                for _ in range(num_heads)\n            ]\n        )\n\n    def forward(self, x):\n        # just apply single-head attentions and concatenate their outputs\n        return torch.cat([head(x) for head in self.heads], dim=-1)", "code_hash": "fc9ba024a0926ae03094b0c442fe0c53", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "WfYj", "name": "*MultiHeadAttentionWrapper"}, {"code": "mo.md(r\"\"\"\nThe output shape is $(B,N,d_{out}\\times2)$.\n\"\"\")", "code_hash": "74de8b12717dac7c4047362a9e77c603", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Ynfw", "name": "_"}, {"code": "torch.manual_seed(123)\n\n_context_length = batch.shape[1]  # This is the number of tokens\n_d_in, _d_out = 3, 2\n_mha = MultiHeadAttentionWrapper(_d_in, _d_out, _context_length, 0.0, num_heads=2)\n\n_context_vecs = _mha(batch)\nprint(_context_vecs)\nprint(f\"{batch.shape=}\")\nprint(f\"{_context_vecs.shape=}\")", "code_hash": "d134ae6b30e0ea1ecbb13817b5eddce2", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "LqFA", "name": "_"}, {"code": "mo.md(r\"\"\"\n### 3.6.2 Implementing multi-head attention with weight splits\n\"\"\")", "code_hash": "d47cb323919be524729487e24e60f574", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "uDnK", "name": "_"}, {"code": "mo.md(r\"\"\"\nSimplify the multi-head attention layer by self-contained way and avoid loops by using matrix products.\n\"\"\")", "code_hash": "87280c496fc98980d01ab0d04ff7b979", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "aWBL", "name": "_"}, {"code": "class MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = (\n            d_out // num_heads\n        )  # Reduce the projection dim to match desired output dim\n\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n        self.dropout = nn.Dropout(dropout)\n        # constant parameters\n        self.register_buffer(\n            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n        )\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`,\n        # this will result in errors in the mask creation further below.\n        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs\n        # do not exceed `context_length` before reaching this forward method.\n\n        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # We implicitly split the matrix by adding a `num_heads` dimension\n        # Unroll last dim: (b, num_tokens, d_out) -\u003E (b, num_tokens, num_heads, head_dim)\n        # So, d_out = num_heads * head_dim (see this constructor)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_tokens, num_heads, head_dim) -\u003E (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1, 2)\n        queries = queries.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n        # Shape: (b, num_heads, num_tokens, num_tokens)\n        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n\n        # Original mask truncated to the number of tokens and converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # Use the mask to fill attention scores\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Shape: (b, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2)\n\n        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec)  # optional projection\n\n        return context_vec", "code_hash": "67e01e5830f6d0c8893c0457ef7302ad", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "MIsd", "name": "*MultiHeadAttention"}, {"code": "torch.manual_seed(123)\n\nbatch_size, context_length, _d_in = batch.shape\n_d_out = 2\nmha = MultiHeadAttention(_d_in, _d_out, context_length, 0.0, num_heads=2)\n\ncontext_vecs = mha(batch)\n\nprint(context_vecs)\nprint(\"context_vecs.shape:\", context_vecs.shape)", "code_hash": "2547ff1060a2f35548ccb196faa2f385", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "IrqS", "name": "_"}, {"code": "# (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4)\na = torch.tensor(\n    [\n        [\n            [\n                [0.2745, 0.6584, 0.2775, 0.8573],\n                [0.8993, 0.0390, 0.9268, 0.7388],\n                [0.7179, 0.7058, 0.9156, 0.4340],\n            ],\n            [\n                [0.0772, 0.3565, 0.1479, 0.5331],\n                [0.4066, 0.2318, 0.4545, 0.9737],\n                [0.4606, 0.5159, 0.4220, 0.5786],\n            ],\n        ]\n    ]\n)\n\nprint(a @ a.transpose(2, 3))", "code_hash": "680e93bc77686ed83720a763ef3e68a5", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Lpqv", "name": "_"}, {"code": "_first_head = a[0, 0, :, :]\nfirst_res = _first_head @ _first_head.T\nprint(\"First head:\\n\", first_res)\n\n_second_head = a[0, 1, :, :]\n_second_res = _second_head @ _second_head.T\nprint(\"\\nSecond head:\\n\", _second_res)", "code_hash": "ae4d3ffaad860d92855d7acf4f61c8ce", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "upgv", "name": "_"}], "metadata": {"marimo_version": "0.18.4"}, "version": "1"},
            "session": {"cells": [{"code_hash": "f80cd351b905c364088fa51d0559db39", "console": [], "id": "setup", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "356c566b8e57516ed4dbea29c5388573", "console": [], "id": "Hbol", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch1 id=\"chapter-3-coding-attention-mechanisms\"\u003EChapter 3: Coding Attention Mechanisms\u003C/h1\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "ac76a1258bd3ba7aa8436da2d786332e", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "torch version: 2.9.1+cu128\n", "type": "stream"}], "id": "MJUe", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "1b72de0e56ac0f117e6e6df8195f0789", "console": [], "id": "vblA", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"33-attending-to-different-parts-of-the-input-with-self-attention\"\u003E3.3 Attending to different parts of the input with self-attention\u003C/h2\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "e6d4c34d291262167f7fd630fd51f17e", "console": [], "id": "bkHC", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"331-a-simple-self-attention-mechanism-without-trainable-weights\"\u003E3.3.1 A simple self-attention mechanism without trainable weights\u003C/h3\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "4ecdd60550fc95574f958fec3932c248", "console": [], "id": "lEQa", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EPrepare embedded vectors as inputs to attention layers.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "47255886eacf9cf4f4a947a0778c9134", "console": [], "id": "PKri", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "a0c210f2a0721e6d4c4b53078999e77f", "console": [], "id": "Xref", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EAttenstion score \u003Cmarimo-tex class=\"arithmatex\"\u003E||(\\omega_{2i}||)\u003C/marimo-tex\u003E is defined as\n$$\n\\vec{\\omega}_{2i} \\equiv \\vec{x}_2 \\cdot \\vec{x}_i.\n$$\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "577bdd9505b0d5e67c0217711991718b", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n", "type": "stream"}], "id": "SFPL", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "8cc4d5403ad6581e162fe069f8a90c29", "console": [], "id": "BYtC", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThis is a verification of dot product for beginners by\n$$\n\\vec{x}\u003Cem\u003E0\\cdot\\vec{x}_1\\equiv\\sum\u003C/em\u003E{i=0}^{\\mathrm{dim}(\\vec{x}_0)}(x_0)_i(x_1)_i.\n$$\nThe dot product has meaning of similarity between two vectors.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "b90b7a557ea42d0cea739437f307bdb9", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor(0.9544)\ntensor(0.9544)\n", "type": "stream"}], "id": "RGSE", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "bef957f8fa5ba9de673528083ce46430", "console": [], "id": "Kclp", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThe attention should be normalized to represents weights.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "b986af97de8771c78b2b6d7f034caf64", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\nSum: tensor(1.0000)\n", "type": "stream"}], "id": "emfo", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "15e9e2fffafd9fbec97dc15aff923e36", "console": [], "id": "Hstk", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EMore suitable function for normalization is the softmax function that has positive values and robust gradient for traning\n$$\n\\sigma(x_i;\\vec{x})\\equiv\\frac{e^{x_i}}{\\sum_j e^{x_j}}.\n$$\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "01d426fe90a7f1e71cc40ff0f1439ba8", "console": [], "id": "nWHF", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "c8f5a9fac3ab502b245a1c4f3008d817", "console": [], "id": "iLit", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThe softmax function also leads weights whose total is 1.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "9b0c68b9ed02b40ee9e41a2a5f733fff", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nSum: tensor(1.)\n", "type": "stream"}], "id": "ZHCJ", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "fd4c9039872fafbdd63653ce5fe2325f", "console": [], "id": "ROlb", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EPyTorch softmax function is more stable with respect to numerical errors because it uses \u003Ca href=\"https://discuss.pytorch.org/t/justification-for-logsoftmax-being-better-than-log-softmax/140130/3\" rel=\"noopener noreferrer\" target=\"_blank\"\u003Emax-trick and LogSumExp trick\u003C/a\u003E.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "46ee78f8dcf59c377491736e1dc64755", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nSum: tensor(1.)\n", "type": "stream"}], "id": "qnkX", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "4a1e95210a8d2650d158b2663e404299", "console": [], "id": "TqIu", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThe attention scores are used as weights for weighted average of embedded vectors.\nThe result is called as context vectors \u003Cmarimo-tex class=\"arithmatex\"\u003E||(\\vec{z}^{(i)}||)\u003C/marimo-tex\u003E.\u003C/span\u003E\n\u003Cspan class=\"paragraph\"\u003EFormaly, the context vectors \u003Cmarimo-tex class=\"arithmatex\"\u003E||(\\vec{z}^{(i)}||)\u003C/marimo-tex\u003E are defined as\n$$\n\\vec{z}^{(i)}\n\\equiv\\sum_j\\mathrm{Softmax}\u003Cem\u003Ej(\\omega\u003C/em\u003E{ij})\\vec{x}\u003Cem\u003Ej\n\\equiv\\frac{\\sum_j\\omega\u003C/em\u003E{ij}\\vec{x}\u003Cem\u003Ej}{\\sum_k\\omega\u003C/em\u003E{ik}}\n\\equiv\\frac{(\\sum_j\\vec{x}_j\\vec{x}_j^T)\\vec{x}_i}{\\sum_k\\vec{x}_k\\cdot\\vec{x}_i}\n$$\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "b10b78de36e39ff699f81a2954584990", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([0.4419, 0.6515, 0.5683])\n", "type": "stream"}], "id": "Vxnm", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "a4847ec0a2934b837c4d8285d9636c51", "console": [], "id": "DnEU", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"332-computing-attention-weights-for-all-input-tokens\"\u003E3.3.2 Computing attention weights for all input tokens\u003C/h3\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "7230e61f3e5b41e543c00e367b2d5838", "console": [], "id": "ulZA", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003ECalculate context vectors with respect to all query vectors.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "f6580dd1727c68985a22f6638e834747", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n", "type": "stream"}], "id": "ecfG", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "a607bd81dc431c54bcedb110e5407a15", "console": [], "id": "Pvdt", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EIn Python, such double for-loops are inefficient. The following matrix product is preferred for computation efficiency. This result is the same with the result of the previous cell.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "a725b139ea3d9bdedddff16283b461a4", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n", "type": "stream"}], "id": "ZBYS", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "8815bafad990234ae36eec3e5c1dc746", "console": [], "id": "aLJB", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EWe can apply softmax function to each rows.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "608f1b575147585791ed731dd36f25f3", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n", "type": "stream"}], "id": "nHfw", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "32daaa92469d6bf75d5eb6b77a77751f", "console": [], "id": "xXTn", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThe normalization is verified by this.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "445458740f1514ef2df8a3a72339717b", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "Row 2 sum: 1.0\nAll row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n", "type": "stream"}], "id": "AjVT", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "554c86000db51fe8d8c440887f0e6847", "console": [], "id": "pHFh", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EWeighted averages are also simplified by using matrix-vector products.\nFinally, we obtain the following formula:\n$$\nZ\\equiv\\mathrm{Softmax}(XX^T)X\n$$\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "a1fe05bbceaec97ebd020e2ebcdda449", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[0.4421, 0.5931, 0.5790],\n        [0.4419, 0.6515, 0.5683],\n        [0.4431, 0.6496, 0.5671],\n        [0.4304, 0.6298, 0.5510],\n        [0.4671, 0.5910, 0.5266],\n        [0.4177, 0.6503, 0.5645]])\n", "type": "stream"}], "id": "NCOB", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "b474df4ce89eec717e1844456c3fef54", "console": [], "id": "aqbW", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThe 2nd slice is the exactly same with the result of the previous section.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "1c9880bf9457187d957fbc550117ca2f", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n", "type": "stream"}], "id": "TRpd", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "b63f1b7cd636c0ab92ce784323a145c4", "console": [], "id": "TXez", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"34-implementing-self-attention-with-trainable-weights\"\u003E3.4 Implementing self-attention with trainable weights\u003C/h2\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "c6405f6ef21ed490604fe1accb5aadb9", "console": [], "id": "dNNg", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"341-computing-the-attention-weights-step-by-step\"\u003E3.4.1 Computing the attention weights step by step\u003C/h3\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "b221b970b84206a7e7d0f3720392b136", "console": [], "id": "yCnT", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EUse these inputs and parameters to show how to introduce trainable attention layer.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "d287737fd5b0281e7d4ba526063d9f1a", "console": [], "id": "wlCL", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "64582f38215fdaef16ce338fd647579b", "console": [], "id": "kqZH", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThese are trainable weight matricies (or linear projections).\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "2cfd47f0efcc4ebf47ed1304b236a81a", "console": [], "id": "wAgl", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "faddb0748ea709832a0c599e42044372", "console": [], "id": "rEll", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThe linear projection are like these\n$$\nQ_2\\equiv \\vec{x}_2 W^Q ,\\quad\nK_2\\equiv \\vec{x}_2 W^K,\\quad\nV_2\\equiv \\vec{x}_2 W^V,\n$$\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "8a0ab3e2f2061cb981d40528f2f95032", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([0.4306, 1.4551])\n", "type": "stream"}], "id": "dGlV", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "ce116d1a8afeb02038a2ef38dab7fb66", "console": [], "id": "SdmI", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003ELinear projection for all inputs are\n$$\nK=XW^K, V=XW^V.\n$$\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "7e134a8474ea63bd5d14de15a2913e13", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "keys.shape: torch.Size([6, 2])\nvalues.shape: torch.Size([6, 2])\n", "type": "stream"}], "id": "lgWD", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "8425abd660941ebfbcf4854c753a222e", "console": [], "id": "yOPj", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThe attention score \u003Cmarimo-tex class=\"arithmatex\"\u003E||(\\omega_{22}\\equiv Q_2\\cdot K_2||)\u003C/marimo-tex\u003E is this.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "e0ac41aaeb3878ee9e6d339ebfd03195", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor(1.8524)\n", "type": "stream"}], "id": "fwwy", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "6a99a6019b551355313b8454ca548f8e", "console": [], "id": "LJZf", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EScores for all keys are calculated by \u003Cmarimo-tex class=\"arithmatex\"\u003E||(\\omega_2\\equiv Q_2 K||)\u003C/marimo-tex\u003E.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "79c4c8a1de94c71f29e7a979d5adcebd", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n", "type": "stream"}], "id": "urSm", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "2b230a782e39bd6d198910008becbca7", "console": [], "id": "jxvo", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003ETake the attention scores by Scaled Dot-Product Attention way.\nThe attention score is scaled by square root of the dimension of keys like\n$$\n\\alpha_2\\equiv\\mathrm{Softmax}\\frac{\\omega_2}{\\sqrt{\\mathrm{dim}(K_i)}}\n$$\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "2fbca3f0d4f9861a1165a136de3feaf2", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n", "type": "stream"}], "id": "mWxS", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "28021fa9d228d4bc2dcb9f772c1154e7", "console": [], "id": "CcZR", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EFinally, we get trainable context vector as\n$$\n\\vec{z}^2\\equiv\\mathrm{Softmax}\\left(\\frac{Q_2K}{\\sqrt{\\mathrm{dim}(K_i)}}\\right)V.\n$$\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "84d74d9186e3058175dd86fce965c7bf", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([0.3061, 0.8210])\n", "type": "stream"}], "id": "YWSi", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "7cd1c33c5e9d4edaa4e095c5f8bc4db8", "console": [], "id": "zlud", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"342-implementing-a-compact-selfattention-class\"\u003E3.4.2 Implementing a compact SelfAttention class\u003C/h3\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "fd813f7a49bc8a40cbcd578baa9374c2", "console": [], "id": "tZnO", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "71a30beabb5fb21930dfc17c9daeeef3", "console": [], "id": "xvXZ", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThe trainable context vectors are calculated by this instance. The 2nd slice of the result is the same with the result of the previous section.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "22fb93a91eb5d37f74addf87800a5af5", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[0.2996, 0.8053],\n        [0.3061, 0.8210],\n        [0.3058, 0.8203],\n        [0.2948, 0.7939],\n        [0.2927, 0.7891],\n        [0.2990, 0.8040]], grad_fn=\u003CMmBackward0\u003E)\n", "type": "stream"}], "id": "CLip", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "841d804357b2a390dac971bb3280646b", "console": [], "id": "YECM", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EBy using \u003Ccode\u003Enn.Linear\u003C/code\u003E, the class definition is more simplified and more stable because of a good initialization scheme called as \u003Ca href=\"https://docs.pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_\" rel=\"noopener noreferrer\" target=\"_blank\"\u003Ethe Kaiming Uniform initialization\u003C/a\u003E.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "96c60e91e0d16427007e35a4e2af54be", "console": [], "id": "cEAS", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "9f88dca8b8156f32cc4925bcac5e18f1", "console": [], "id": "iXej", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThis output is different with the previous results because of such initialization schemes.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "f60c3691241f0e09c605f83b01c3cbf2", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[-0.0739,  0.0713],\n        [-0.0748,  0.0703],\n        [-0.0749,  0.0702],\n        [-0.0760,  0.0685],\n        [-0.0763,  0.0679],\n        [-0.0754,  0.0693]], grad_fn=\u003CMmBackward0\u003E)\n", "type": "stream"}], "id": "EJmg", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "8bf3ac6400f1ff6082dab26d161ab918", "console": [], "id": "UmEG", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"35-hiding-future-words-with-causal-attention\"\u003E3.5 Hiding future words with causal attention\u003C/h2\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "57c511ce00e971f7be359be966819ceb", "console": [], "id": "vEBW", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"351-applying-a-causal-attention-mask\"\u003E3.5.1 Applying a causal attention mask\u003C/h3\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "aa7e4b02aa0288ed425038ca02e54c17", "console": [], "id": "kLmu", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThis is an initial attention weights like previous.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "cd842cbe5b737cff15c08229cf06e56b", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n       grad_fn=\u003CSoftmaxBackward0\u003E)\n", "type": "stream"}], "id": "IpqN", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "afc1416a20a22e1bbf523a288e02b85c", "console": [], "id": "dxZZ", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003ELet's mask the upper right of it to ensure causal token prediction tasks.\nSuch binary mask is easily created by \u003Ccode\u003Etorch.tril\u003C/code\u003E.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "54ba0591bc55a74e73ed905c165099af", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[1., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1.]])\n", "type": "stream"}], "id": "dlnW", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "444d50edf93def25bb0fb23ddd306f33", "console": [], "id": "TTti", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThe masking is simple multiplication of these \u003Cmarimo-tex class=\"arithmatex\"\u003E||(W_a\\mathbb{1}_{i\\geq j}||)\u003C/marimo-tex\u003E.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "a933b55c5e91dfd6e2e18a78100fdf29", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[0.2098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1385, 0.2379, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1390, 0.2369, 0.2326, 0.0000, 0.0000, 0.0000],\n        [0.1435, 0.2074, 0.2046, 0.1462, 0.0000, 0.0000],\n        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.0000],\n        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n", "type": "stream"}], "id": "RKFZ", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "4955678db132bc3de765fb3feaa099e7", "console": [], "id": "IaQp", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003ENormalize each rows to use it as weights. The normalization should be done after the masking to prevent data leak from futures.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "5999220bcce3b0bff4ac0e1c7d141703", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3680, 0.6320, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2284, 0.3893, 0.3822, 0.0000, 0.0000, 0.0000],\n        [0.2046, 0.2956, 0.2915, 0.2084, 0.0000, 0.0000],\n        [0.1753, 0.2250, 0.2269, 0.1570, 0.2158, 0.0000],\n        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n", "type": "stream"}], "id": "IWgg", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "3ab7ecedefc7269da93d6e3beaf4205c", "console": [], "id": "fCoF", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EFor more efficiency, this masking is replaced by filling \u003Ccode\u003E-inf\u003C/code\u003E to upper right of attention scores before taking Softmax.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "b6fcb9fefa5df01ebac9465b12cee6a3", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[0.9995,   -inf,   -inf,   -inf,   -inf,   -inf],\n        [0.9544, 1.4950,   -inf,   -inf,   -inf,   -inf],\n        [0.9422, 1.4754, 1.4570,   -inf,   -inf,   -inf],\n        [0.4753, 0.8434, 0.8296, 0.4937,   -inf,   -inf],\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654,   -inf],\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n", "type": "stream"}], "id": "LkGn", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "e3421f9caf20181d4d21929c376b0742", "console": [], "id": "zVRe", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003E\u003Cmarimo-tex class=\"arithmatex\"\u003E||(\\exp(-\\infty)=0||)\u003C/marimo-tex\u003E leads the same result with the masking.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "5d3313b0e89b9adfa164412dc0ad3f55", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.4056, 0.5944, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2566, 0.3741, 0.3693, 0.0000, 0.0000, 0.0000],\n        [0.2176, 0.2823, 0.2796, 0.2205, 0.0000, 0.0000],\n        [0.1826, 0.2178, 0.2191, 0.1689, 0.2115, 0.0000],\n        [0.1473, 0.2033, 0.1996, 0.1500, 0.1160, 0.1839]])\n", "type": "stream"}], "id": "woaO", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "b251d41af776af72e8ff16cba385925b", "console": [], "id": "HnMC", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"352-masking-additional-attention-weights-with-dropout\"\u003E3.5.2 Masking additional attention weights with dropout\u003C/h3\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "5acd40cccb5ab0b77ce60820056c8db1", "console": [], "id": "wadT", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EWe introduct dropout to prevent overfitting to specific tokens. We drop 50% of elements and the remaining values are doubled to keep the total scale. This dropout layer is applied only for training.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "a52ceb168174043b85cac19d2261b2c6", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[2., 2., 0., 2., 2., 0.],\n        [0., 0., 0., 2., 0., 2.],\n        [2., 2., 2., 2., 0., 2.],\n        [0., 2., 2., 0., 0., 2.],\n        [0., 2., 0., 2., 0., 2.],\n        [0., 2., 2., 2., 2., 0.]])\n", "type": "stream"}], "id": "VCRE", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "2e5d4b307962790c9bb8cac1778ce7ab", "console": [], "id": "hgqU", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThe dropout layer is applied after attention weights are calculated.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "6628c0ce8df424d1a3c72cf295c6c3a8", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[0.4197, 0.4012, 0.0000, 0.2485, 0.2441, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.2480, 0.0000, 0.3162],\n        [0.2780, 0.4738, 0.4652, 0.2484, 0.0000, 0.3129],\n        [0.0000, 0.4148, 0.4091, 0.0000, 0.0000, 0.3441],\n        [0.0000, 0.3917, 0.0000, 0.2734, 0.0000, 0.2590],\n        [0.0000, 0.4367, 0.4255, 0.2841, 0.1976, 0.0000]])\n", "type": "stream"}], "id": "PSUk", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "fc8ed30bfdf2007d29e2ee5c58f10cc0", "console": [], "id": "mfOT", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"353-implementing-a-compact-causal-self-attention-class\"\u003E3.5.3 Implementing a compact causal self-attention class\u003C/h3\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "370c21f9e8ac28cf72329e5926808ab7", "console": [], "id": "vGiW", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EWe introduce such causal attention, dropout, and batch inference to attention layer. The batch input for testing is as follows.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "c247a774e29f9f681cfbd37259eb5810", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "torch.Size([2, 6, 3])\n", "type": "stream"}], "id": "SYQT", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "451d1c657edd1be831d2e531ffb1845b", "console": [], "id": "bMrW", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThis is an example for causal attention layer. \u003Ca href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer\" rel=\"noopener noreferrer\" target=\"_blank\"\u003E\u003Ccode\u003Eregister_buffer\u003C/code\u003E\u003C/a\u003E is used to define constant (non-trained) parameters and ensure it is allocated on suitable device.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "2fb96fa912dd5f93e4408015564d621f", "console": [], "id": "PSQn", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "cf48c59b8820e6b4ac72bee6f5575f52", "console": [], "id": "OfTS", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThis is an example to use the forward path. Check the input and output shapes.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "e7b136ad731c5decb153b4251a64af94", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[[-0.4519,  0.2216],\n         [-0.5874,  0.0058],\n         [-0.6300, -0.0632],\n         [-0.5675, -0.0843],\n         [-0.5526, -0.0981],\n         [-0.5299, -0.1081]],\n\n        [[-0.4519,  0.2216],\n         [-0.5874,  0.0058],\n         [-0.6300, -0.0632],\n         [-0.5675, -0.0843],\n         [-0.5526, -0.0981],\n         [-0.5299, -0.1081]]], grad_fn=\u003CUnsafeViewBackward0\u003E)\nbatch.shape=torch.Size([2, 6, 3])\n_context_vecs.shape=torch.Size([2, 6, 2])\nd_in=3, d_out=2\n", "type": "stream"}], "id": "lQxp", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "6a633c1e552821a320f3a8ccde69baac", "console": [], "id": "Plbk", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch2 id=\"36-extending-single-head-attention-to-multi-head-attention\"\u003E3.6 Extending single-head attention to multi-head attention\u003C/h2\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "f743c24923cb102829cb8d88ecbbdb2c", "console": [], "id": "rSYo", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"361-stacking-multiple-single-head-attention-layers\"\u003E3.6.1 Stacking multiple single-head attention layers\u003C/h3\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "bf56d8bf893de06c4814455909aff84b", "console": [], "id": "HuZB", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003ELet's extend the attention layer to multi-head. This is a naive extetion from single-head one.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "fc9ba024a0926ae03094b0c442fe0c53", "console": [], "id": "WfYj", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "74de8b12717dac7c4047362a9e77c603", "console": [], "id": "Ynfw", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003EThe output shape is \u003Cmarimo-tex class=\"arithmatex\"\u003E||((B,N,d_{out}\\times2)||)\u003C/marimo-tex\u003E.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "d134ae6b30e0ea1ecbb13817b5eddce2", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n         [-0.5874,  0.0058,  0.5891,  0.3257],\n         [-0.6300, -0.0632,  0.6202,  0.3860],\n         [-0.5675, -0.0843,  0.5478,  0.3589],\n         [-0.5526, -0.0981,  0.5321,  0.3428],\n         [-0.5299, -0.1081,  0.5077,  0.3493]],\n\n        [[-0.4519,  0.2216,  0.4772,  0.1063],\n         [-0.5874,  0.0058,  0.5891,  0.3257],\n         [-0.6300, -0.0632,  0.6202,  0.3860],\n         [-0.5675, -0.0843,  0.5478,  0.3589],\n         [-0.5526, -0.0981,  0.5321,  0.3428],\n         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=\u003CCatBackward0\u003E)\nbatch.shape=torch.Size([2, 6, 3])\n_context_vecs.shape=torch.Size([2, 6, 4])\n", "type": "stream"}], "id": "LqFA", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "d47cb323919be524729487e24e60f574", "console": [], "id": "uDnK", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Ch3 id=\"362-implementing-multi-head-attention-with-weight-splits\"\u003E3.6.2 Implementing multi-head attention with weight splits\u003C/h3\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "87280c496fc98980d01ab0d04ff7b979", "console": [], "id": "aWBL", "outputs": [{"data": {"text/markdown": "\u003Cspan class=\"markdown prose dark:prose-invert contents\"\u003E\u003Cspan class=\"paragraph\"\u003ESimplify the multi-head attention layer by self-contained way and avoid loops by using matrix products.\u003C/span\u003E\u003C/span\u003E"}, "type": "data"}]}, {"code_hash": "67e01e5830f6d0c8893c0457ef7302ad", "console": [], "id": "MIsd", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "2547ff1060a2f35548ccb196faa2f385", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[[0.3190, 0.4858],\n         [0.2943, 0.3897],\n         [0.2856, 0.3593],\n         [0.2693, 0.3873],\n         [0.2639, 0.3928],\n         [0.2575, 0.4028]],\n\n        [[0.3190, 0.4858],\n         [0.2943, 0.3897],\n         [0.2856, 0.3593],\n         [0.2693, 0.3873],\n         [0.2639, 0.3928],\n         [0.2575, 0.4028]]], grad_fn=\u003CViewBackward0\u003E)\ncontext_vecs.shape: torch.Size([2, 6, 2])\n", "type": "stream"}], "id": "IrqS", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "680e93bc77686ed83720a763ef3e68a5", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "tensor([[[[1.3208, 1.1631, 1.2879],\n          [1.1631, 2.2150, 1.8424],\n          [1.2879, 1.8424, 2.0402]],\n\n         [[0.4391, 0.7003, 0.5903],\n          [0.7003, 1.3737, 1.0620],\n          [0.5903, 1.0620, 0.9912]]]])\n", "type": "stream"}], "id": "Lpqv", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "ae4d3ffaad860d92855d7acf4f61c8ce", "console": [{"mimetype": "text/plain", "name": "stdout", "text": "First head:\n tensor([[1.3208, 1.1631, 1.2879],\n        [1.1631, 2.2150, 1.8424],\n        [1.2879, 1.8424, 2.0402]])\n\nSecond head:\n tensor([[0.4391, 0.7003, 0.5903],\n        [0.7003, 1.3737, 1.0620],\n        [0.5903, 1.0620, 0.9912]])\n", "type": "stream"}], "id": "upgv", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}], "metadata": {"marimo_version": "0.18.4"}, "version": "1"},
            "runtimeConfig": null,
        };
    </script>
  
<marimo-code hidden="">
    import%20marimo%0A%0A__generated_with%20%3D%20%220.18.4%22%0Aapp%20%3D%20marimo.App(width%3D%22medium%22)%0A%0Awith%20app.setup%3A%0A%20%20%20%20from%20importlib.metadata%20import%20version%0A%0A%20%20%20%20import%20marimo%20as%20mo%0A%20%20%20%20import%20torch%0A%20%20%20%20import%20torch.nn%20as%20nn%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%20Chapter%203%3A%20Coding%20Attention%20Mechanisms%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20print(%22torch%20version%3A%22%2C%20version(%22torch%22))%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%203.3%20Attending%20to%20different%20parts%20of%20the%20input%20with%20self-attention%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%203.3.1%20A%20simple%20self-attention%20mechanism%20without%20trainable%20weights%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20Prepare%20embedded%20vectors%20as%20inputs%20to%20attention%20layers.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20inputs%20%3D%20torch.tensor(%0A%20%20%20%20%20%20%20%20%5B%0A%20%20%20%20%20%20%20%20%20%20%20%20%5B0.43%2C%200.15%2C%200.89%5D%2C%20%20%23%20Your%20%20%20%20%20(x%5E1)%0A%20%20%20%20%20%20%20%20%20%20%20%20%5B0.55%2C%200.87%2C%200.66%5D%2C%20%20%23%20journey%20%20(x%5E2)%0A%20%20%20%20%20%20%20%20%20%20%20%20%5B0.57%2C%200.85%2C%200.64%5D%2C%20%20%23%20starts%20%20%20(x%5E3)%0A%20%20%20%20%20%20%20%20%20%20%20%20%5B0.22%2C%200.58%2C%200.33%5D%2C%20%20%23%20with%20%20%20%20%20(x%5E4)%0A%20%20%20%20%20%20%20%20%20%20%20%20%5B0.77%2C%200.25%2C%200.10%5D%2C%20%20%23%20one%20%20%20%20%20%20(x%5E5)%0A%20%20%20%20%20%20%20%20%20%20%20%20%5B0.05%2C%200.80%2C%200.55%5D%2C%20%20%23%20step%20%20%20%20%20(x%5E6)%0A%20%20%20%20%20%20%20%20%5D%0A%20%20%20%20)%0A%20%20%20%20return%20(inputs%2C)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20Attenstion%20score%20%24%5Comega_%7B2i%7D%24%20is%20defined%20as%0A%20%20%20%20%24%24%0A%20%20%20%20%5Cvec%7B%5Comega%7D_%7B2i%7D%20%5Cequiv%20%5Cvec%7Bx%7D_2%20%5Ccdot%20%5Cvec%7Bx%7D_i.%0A%20%20%20%20%24%24%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(inputs)%3A%0A%20%20%20%20query%20%3D%20inputs%5B1%5D%20%20%23%202nd%20input%20token%20is%20the%20query%0A%0A%20%20%20%20attn_scores_2_trainless%20%3D%20torch.empty(inputs.shape%5B0%5D)%0A%20%20%20%20for%20_i%2C%20_x_i%20in%20enumerate(inputs)%3A%0A%20%20%20%20%20%20%20%20%23%20dot%20product%20(transpose%20not%20necessary%20here%20since%20they%20are%201-dim%20vectors)%0A%20%20%20%20%20%20%20%20attn_scores_2_trainless%5B_i%5D%20%3D%20torch.dot(_x_i%2C%20query)%0A%20%20%20%20print(attn_scores_2_trainless)%0A%20%20%20%20return%20attn_scores_2_trainless%2C%20query%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20This%20is%20a%20verification%20of%20dot%20product%20for%20beginners%20by%0A%20%20%20%20%24%24%0A%20%20%20%20%5Cvec%7Bx%7D_0%5Ccdot%5Cvec%7Bx%7D_1%5Cequiv%5Csum_%7Bi%3D0%7D%5E%7B%5Cmathrm%7Bdim%7D(%5Cvec%7Bx%7D_0)%7D(x_0)_i(x_1)_i.%0A%20%20%20%20%24%24%0A%20%20%20%20The%20dot%20product%20has%20meaning%20of%20similarity%20between%20two%20vectors.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(inputs%2C%20query)%3A%0A%20%20%20%20_res%20%3D%200.0%0A%20%20%20%20for%20_idx%2C%20_element%20in%20enumerate(inputs%5B0%5D)%3A%0A%20%20%20%20%20%20%20%20_res%20%2B%3D%20inputs%5B0%5D%5B_idx%5D%20*%20query%5B_idx%5D%0A%0A%20%20%20%20print(_res)%0A%20%20%20%20print(torch.dot(inputs%5B0%5D%2C%20query))%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20The%20attention%20should%20be%20normalized%20to%20represents%20weights.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(attn_scores_2_trainless)%3A%0A%20%20%20%20_attn_weights_2_tmp%20%3D%20attn_scores_2_trainless%20%2F%20attn_scores_2_trainless.sum()%0A%0A%20%20%20%20print(%22Attention%20weights%3A%22%2C%20_attn_weights_2_tmp)%0A%20%20%20%20print(%22Sum%3A%22%2C%20_attn_weights_2_tmp.sum())%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20More%20suitable%20function%20for%20normalization%20is%20the%20softmax%20function%20that%20has%20positive%20values%20and%20robust%20gradient%20for%20traning%0A%20%20%20%20%24%24%0A%20%20%20%20%5Csigma(x_i%3B%5Cvec%7Bx%7D)%5Cequiv%5Cfrac%7Be%5E%7Bx_i%7D%7D%7B%5Csum_j%20e%5E%7Bx_j%7D%7D.%0A%20%20%20%20%24%24%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.function%0Adef%20softmax_naive(x)%3A%0A%20%20%20%20return%20torch.exp(x)%20%2F%20torch.exp(x).sum(dim%3D0)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20The%20softmax%20function%20also%20leads%20weights%20whose%20total%20is%201.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(attn_scores_2_trainless)%3A%0A%20%20%20%20_attn_weights_2_naive%20%3D%20softmax_naive(attn_scores_2_trainless)%0A%0A%20%20%20%20print(%22Attention%20weights%3A%22%2C%20_attn_weights_2_naive)%0A%20%20%20%20print(%22Sum%3A%22%2C%20_attn_weights_2_naive.sum())%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20PyTorch%20softmax%20function%20is%20more%20stable%20with%20respect%20to%20numerical%20errors%20because%20it%20uses%20%5Bmax-trick%20and%20LogSumExp%20trick%5D(https%3A%2F%2Fdiscuss.pytorch.org%2Ft%2Fjustification-for-logsoftmax-being-better-than-log-softmax%2F140130%2F3).%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(attn_scores_2_trainless)%3A%0A%20%20%20%20attn_weights_2_trainless%20%3D%20torch.softmax(attn_scores_2_trainless%2C%20dim%3D0)%0A%0A%20%20%20%20print(%22Attention%20weights%3A%22%2C%20attn_weights_2_trainless)%0A%20%20%20%20print(%22Sum%3A%22%2C%20attn_weights_2_trainless.sum())%0A%20%20%20%20return%20(attn_weights_2_trainless%2C)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20The%20attention%20scores%20are%20used%20as%20weights%20for%20weighted%20average%20of%20embedded%20vectors.%0A%20%20%20%20The%20result%20is%20called%20as%20context%20vectors%20%24%5Cvec%7Bz%7D%5E%7B(i)%7D%24.%0A%0A%20%20%20%20Formaly%2C%20the%20context%20vectors%20%24%5Cvec%7Bz%7D%5E%7B(i)%7D%24%20are%20defined%20as%0A%20%20%20%20%24%24%0A%20%20%20%20%5Cvec%7Bz%7D%5E%7B(i)%7D%0A%20%20%20%20%5Cequiv%5Csum_j%5Cmathrm%7BSoftmax%7D_j(%5Comega_%7Bij%7D)%5Cvec%7Bx%7D_j%0A%20%20%20%20%5Cequiv%5Cfrac%7B%5Csum_j%5Comega_%7Bij%7D%5Cvec%7Bx%7D_j%7D%7B%5Csum_k%5Comega_%7Bik%7D%7D%0A%20%20%20%20%5Cequiv%5Cfrac%7B(%5Csum_j%5Cvec%7Bx%7D_j%5Cvec%7Bx%7D_j%5ET)%5Cvec%7Bx%7D_i%7D%7B%5Csum_k%5Cvec%7Bx%7D_k%5Ccdot%5Cvec%7Bx%7D_i%7D%0A%20%20%20%20%24%24%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(attn_weights_2_trainless%2C%20inputs%2C%20query)%3A%0A%20%20%20%20context_vec_2_trainless%20%3D%20torch.zeros(query.shape)%0A%20%20%20%20for%20_i%2C%20_x_i%20in%20enumerate(inputs)%3A%0A%20%20%20%20%20%20%20%20context_vec_2_trainless%20%2B%3D%20attn_weights_2_trainless%5B_i%5D%20*%20_x_i%0A%0A%20%20%20%20print(context_vec_2_trainless)%0A%20%20%20%20return%20(context_vec_2_trainless%2C)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%203.3.2%20Computing%20attention%20weights%20for%20all%20input%20tokens%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20Calculate%20context%20vectors%20with%20respect%20to%20all%20query%20vectors.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(inputs)%3A%0A%20%20%20%20_attn_scores%20%3D%20torch.empty(6%2C%206)%0A%0A%20%20%20%20for%20_i%2C%20_x_i%20in%20enumerate(inputs)%3A%0A%20%20%20%20%20%20%20%20for%20_j%2C%20_x_j%20in%20enumerate(inputs)%3A%0A%20%20%20%20%20%20%20%20%20%20%20%20_attn_scores%5B_i%2C%20_j%5D%20%3D%20torch.dot(_x_i%2C%20_x_j)%0A%20%20%20%20print(_attn_scores)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20In%20Python%2C%20such%20double%20for-loops%20are%20inefficient.%20The%20following%20matrix%20product%20is%20preferred%20for%20computation%20efficiency.%20This%20result%20is%20the%20same%20with%20the%20result%20of%20the%20previous%20cell.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(inputs)%3A%0A%20%20%20%20attn_scores%20%3D%20inputs%20%40%20inputs.T%0A%20%20%20%20print(attn_scores)%0A%20%20%20%20return%20(attn_scores%2C)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20We%20can%20apply%20softmax%20function%20to%20each%20rows.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(attn_scores)%3A%0A%20%20%20%20attn_weights%20%3D%20torch.softmax(attn_scores%2C%20dim%3D-1)%0A%20%20%20%20print(attn_weights)%0A%20%20%20%20return%20(attn_weights%2C)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20The%20normalization%20is%20verified%20by%20this.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(attn_weights)%3A%0A%20%20%20%20row_2_sum%20%3D%20sum(%5B0.1385%2C%200.2379%2C%200.2333%2C%200.1240%2C%200.1082%2C%200.1581%5D)%0A%20%20%20%20print(%22Row%202%20sum%3A%22%2C%20row_2_sum)%0A%0A%20%20%20%20print(%22All%20row%20sums%3A%22%2C%20attn_weights.sum(dim%3D-1))%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20Weighted%20averages%20are%20also%20simplified%20by%20using%20matrix-vector%20products.%0A%20%20%20%20Finally%2C%20we%20obtain%20the%20following%20formula%3A%0A%20%20%20%20%24%24%0A%20%20%20%20Z%5Cequiv%5Cmathrm%7BSoftmax%7D(XX%5ET)X%0A%20%20%20%20%24%24%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(attn_weights%2C%20inputs)%3A%0A%20%20%20%20all_context_vecs%20%3D%20attn_weights%20%40%20inputs%0A%20%20%20%20print(all_context_vecs)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20The%202nd%20slice%20is%20the%20exactly%20same%20with%20the%20result%20of%20the%20previous%20section.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(context_vec_2_trainless)%3A%0A%20%20%20%20print(%22Previous%202nd%20context%20vector%3A%22%2C%20context_vec_2_trainless)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%203.4%20Implementing%20self-attention%20with%20trainable%20weights%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%203.4.1%20Computing%20the%20attention%20weights%20step%20by%20step%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20Use%20these%20inputs%20and%20parameters%20to%20show%20how%20to%20introduce%20trainable%20attention%20layer.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(inputs)%3A%0A%20%20%20%20x_2%20%3D%20inputs%5B1%5D%20%20%23%20second%20input%20element%0A%20%20%20%20d_in%20%3D%20inputs.shape%5B1%5D%20%20%23%20the%20input%20embedding%20size%2C%20d%3D3%0A%20%20%20%20d_out%20%3D%202%20%20%23%20the%20output%20embedding%20size%2C%20d%3D2%0A%20%20%20%20return%20d_in%2C%20d_out%2C%20x_2%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20These%20are%20trainable%20weight%20matricies%20(or%20linear%20projections).%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(d_in%2C%20d_out)%3A%0A%20%20%20%20torch.manual_seed(123)%0A%0A%20%20%20%20%23%20requires_grad%3DFalse%20is%20for%20simplification%20purposes%20only%20(True%20is%20required%20for%20training)%0A%20%20%20%20W_query%20%3D%20torch.nn.Parameter(torch.rand(d_in%2C%20d_out)%2C%20requires_grad%3DFalse)%0A%20%20%20%20W_key%20%3D%20torch.nn.Parameter(torch.rand(d_in%2C%20d_out)%2C%20requires_grad%3DFalse)%0A%20%20%20%20W_value%20%3D%20torch.nn.Parameter(torch.rand(d_in%2C%20d_out)%2C%20requires_grad%3DFalse)%0A%20%20%20%20return%20W_key%2C%20W_query%2C%20W_value%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20The%20linear%20projection%20are%20like%20these%0A%20%20%20%20%24%24%0A%20%20%20%20Q_2%5Cequiv%20%5Cvec%7Bx%7D_2%20W%5EQ%20%2C%5Cquad%0A%20%20%20%20K_2%5Cequiv%20%5Cvec%7Bx%7D_2%20W%5EK%2C%5Cquad%0A%20%20%20%20V_2%5Cequiv%20%5Cvec%7Bx%7D_2%20W%5EV%2C%0A%20%20%20%20%24%24%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(W_key%2C%20W_query%2C%20W_value%2C%20x_2)%3A%0A%20%20%20%20query_2%20%3D%20x_2%20%40%20W_query%20%20%23%20_2%20because%20it's%20with%20respect%20to%20the%202nd%20input%20element%0A%20%20%20%20key_2%20%3D%20x_2%20%40%20W_key%0A%20%20%20%20value_2%20%3D%20x_2%20%40%20W_value%0A%0A%20%20%20%20print(query_2)%0A%20%20%20%20return%20(query_2%2C)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20Linear%20projection%20for%20all%20inputs%20are%0A%20%20%20%20%24%24%0A%20%20%20%20K%3DXW%5EK%2C%20V%3DXW%5EV.%0A%20%20%20%20%24%24%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(W_key%2C%20W_value%2C%20inputs)%3A%0A%20%20%20%20keys%20%3D%20inputs%20%40%20W_key%0A%20%20%20%20values%20%3D%20inputs%20%40%20W_value%0A%0A%20%20%20%20print(%22keys.shape%3A%22%2C%20keys.shape)%0A%20%20%20%20print(%22values.shape%3A%22%2C%20values.shape)%0A%20%20%20%20return%20keys%2C%20values%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20The%20attention%20score%20%24%5Comega_%7B22%7D%5Cequiv%20Q_2%5Ccdot%20K_2%24%20is%20this.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(keys%2C%20query_2)%3A%0A%20%20%20%20keys_2%20%3D%20keys%5B1%5D%20%20%23%20Python%20starts%20index%20at%200%0A%20%20%20%20attn_score_22%20%3D%20query_2.dot(keys_2)%0A%20%20%20%20print(attn_score_22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20Scores%20for%20all%20keys%20are%20calculated%20by%20%24%5Comega_2%5Cequiv%20Q_2%20K%24.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(keys%2C%20query_2)%3A%0A%20%20%20%20attn_scores_2%20%3D%20query_2%20%40%20keys.T%20%20%23%20All%20attention%20scores%20for%20given%20query%0A%20%20%20%20print(attn_scores_2)%0A%20%20%20%20return%20(attn_scores_2%2C)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20Take%20the%20attention%20scores%20by%20Scaled%20Dot-Product%20Attention%20way.%0A%20%20%20%20The%20attention%20score%20is%20scaled%20by%20square%20root%20of%20the%20dimension%20of%20keys%20like%0A%20%20%20%20%24%24%0A%20%20%20%20%5Calpha_2%5Cequiv%5Cmathrm%7BSoftmax%7D%5Cfrac%7B%5Comega_2%7D%7B%5Csqrt%7B%5Cmathrm%7Bdim%7D(K_i)%7D%7D%0A%20%20%20%20%24%24%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(attn_scores_2%2C%20keys)%3A%0A%20%20%20%20d_k%20%3D%20keys.shape%5B1%5D%0A%20%20%20%20attn_weights_2%20%3D%20torch.softmax(attn_scores_2%20%2F%20d_k**0.5%2C%20dim%3D-1)%0A%20%20%20%20print(attn_weights_2)%0A%20%20%20%20return%20(attn_weights_2%2C)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20Finally%2C%20we%20get%20trainable%20context%20vector%20as%0A%20%20%20%20%24%24%0A%20%20%20%20%5Cvec%7Bz%7D%5E2%5Cequiv%5Cmathrm%7BSoftmax%7D%5Cleft(%5Cfrac%7BQ_2K%7D%7B%5Csqrt%7B%5Cmathrm%7Bdim%7D(K_i)%7D%7D%5Cright)V.%0A%20%20%20%20%24%24%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(attn_weights_2%2C%20values)%3A%0A%20%20%20%20context_vec_2%20%3D%20attn_weights_2%20%40%20values%0A%20%20%20%20print(context_vec_2)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%203.4.2%20Implementing%20a%20compact%20SelfAttention%20class%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.class_definition%0Aclass%20SelfAttention_v1(nn.Module)%3A%0A%20%20%20%20def%20__init__(self%2C%20d_in%2C%20d_out)%3A%0A%20%20%20%20%20%20%20%20super().__init__()%0A%20%20%20%20%20%20%20%20%23%20trainable%20parameters%20are%20stored%20as%20member%20variables%0A%20%20%20%20%20%20%20%20self.W_query%20%3D%20nn.Parameter(torch.rand(d_in%2C%20d_out))%0A%20%20%20%20%20%20%20%20self.W_key%20%3D%20nn.Parameter(torch.rand(d_in%2C%20d_out))%0A%20%20%20%20%20%20%20%20self.W_value%20%3D%20nn.Parameter(torch.rand(d_in%2C%20d_out))%0A%0A%20%20%20%20def%20forward(self%2C%20x)%3A%0A%20%20%20%20%20%20%20%20keys%20%3D%20x%20%40%20self.W_key%0A%20%20%20%20%20%20%20%20queries%20%3D%20x%20%40%20self.W_query%0A%20%20%20%20%20%20%20%20values%20%3D%20x%20%40%20self.W_value%0A%0A%20%20%20%20%20%20%20%20attn_scores%20%3D%20queries%20%40%20keys.T%20%20%23%20omega%0A%20%20%20%20%20%20%20%20attn_weights%20%3D%20torch.softmax(attn_scores%20%2F%20keys.shape%5B-1%5D%20**%200.5%2C%20dim%3D-1)%0A%0A%20%20%20%20%20%20%20%20context_vec%20%3D%20attn_weights%20%40%20values%0A%20%20%20%20%20%20%20%20return%20context_vec%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20The%20trainable%20context%20vectors%20are%20calculated%20by%20this%20instance.%20The%202nd%20slice%20of%20the%20result%20is%20the%20same%20with%20the%20result%20of%20the%20previous%20section.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(d_in%2C%20d_out%2C%20inputs)%3A%0A%20%20%20%20torch.manual_seed(123)%0A%20%20%20%20sa_v1%20%3D%20SelfAttention_v1(d_in%2C%20d_out)%0A%20%20%20%20print(sa_v1(inputs))%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20By%20using%20%60nn.Linear%60%2C%20the%20class%20definition%20is%20more%20simplified%20and%20more%20stable%20because%20of%20a%20good%20initialization%20scheme%20called%20as%20%5Bthe%20Kaiming%20Uniform%20initialization%5D(https%3A%2F%2Fdocs.pytorch.org%2Fdocs%2Fstable%2Fnn.init.html%23torch.nn.init.kaiming_uniform_).%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.class_definition%0Aclass%20SelfAttention_v2(nn.Module)%3A%0A%20%20%20%20def%20__init__(self%2C%20d_in%2C%20d_out%2C%20qkv_bias%3DFalse)%3A%0A%20%20%20%20%20%20%20%20super().__init__()%0A%20%20%20%20%20%20%20%20self.W_query%20%3D%20nn.Linear(d_in%2C%20d_out%2C%20bias%3Dqkv_bias)%0A%20%20%20%20%20%20%20%20self.W_key%20%3D%20nn.Linear(d_in%2C%20d_out%2C%20bias%3Dqkv_bias)%0A%20%20%20%20%20%20%20%20self.W_value%20%3D%20nn.Linear(d_in%2C%20d_out%2C%20bias%3Dqkv_bias)%0A%0A%20%20%20%20def%20forward(self%2C%20x)%3A%0A%20%20%20%20%20%20%20%20keys%20%3D%20self.W_key(x)%0A%20%20%20%20%20%20%20%20queries%20%3D%20self.W_query(x)%0A%20%20%20%20%20%20%20%20values%20%3D%20self.W_value(x)%0A%0A%20%20%20%20%20%20%20%20attn_scores%20%3D%20queries%20%40%20keys.T%0A%20%20%20%20%20%20%20%20attn_weights%20%3D%20torch.softmax(attn_scores%20%2F%20keys.shape%5B-1%5D%20**%200.5%2C%20dim%3D-1)%0A%0A%20%20%20%20%20%20%20%20context_vec%20%3D%20attn_weights%20%40%20values%0A%20%20%20%20%20%20%20%20return%20context_vec%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20This%20output%20is%20different%20with%20the%20previous%20results%20because%20of%20such%20initialization%20schemes.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(d_in%2C%20d_out%2C%20inputs)%3A%0A%20%20%20%20torch.manual_seed(789)%0A%20%20%20%20sa_v2%20%3D%20SelfAttention_v2(d_in%2C%20d_out)%0A%20%20%20%20print(sa_v2(inputs))%0A%20%20%20%20return%20(sa_v2%2C)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%203.5%20Hiding%20future%20words%20with%20causal%20attention%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%203.5.1%20Applying%20a%20causal%20attention%20mask%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20This%20is%20an%20initial%20attention%20weights%20like%20previous.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(inputs%2C%20sa_v2)%3A%0A%20%20%20%20%23%20Reuse%20the%20query%20and%20key%20weight%20matrices%20of%20the%0A%20%20%20%20%23%20SelfAttention_v2%20object%20from%20the%20previous%20section%20for%20convenience%0A%20%20%20%20_queries%20%3D%20sa_v2.W_query(inputs)%0A%20%20%20%20_keys%20%3D%20sa_v2.W_key(inputs)%0A%20%20%20%20_attn_scores%20%3D%20_queries%20%40%20_keys.T%0A%0A%20%20%20%20attn_weights_sa_v2%20%3D%20torch.softmax(_attn_scores%20%2F%20_keys.shape%5B-1%5D%20**%200.5%2C%20dim%3D-1)%0A%20%20%20%20print(attn_weights_sa_v2)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20Let's%20mask%20the%20upper%20right%20of%20it%20to%20ensure%20causal%20token%20prediction%20tasks.%0A%20%20%20%20Such%20binary%20mask%20is%20easily%20created%20by%20%60torch.tril%60.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(attn_scores)%3A%0A%20%20%20%20_context_length%20%3D%20attn_scores.shape%5B0%5D%0A%20%20%20%20mask_simple%20%3D%20torch.tril(torch.ones(_context_length%2C%20_context_length))%0A%20%20%20%20print(mask_simple)%0A%20%20%20%20return%20(mask_simple%2C)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20The%20masking%20is%20simple%20multiplication%20of%20these%20%24W_a%5Cmathbb%7B1%7D_%7Bi%5Cgeq%20j%7D%24.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(attn_weights%2C%20mask_simple)%3A%0A%20%20%20%20masked_simple%20%3D%20attn_weights%20*%20mask_simple%0A%20%20%20%20print(masked_simple)%0A%20%20%20%20return%20(masked_simple%2C)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20Normalize%20each%20rows%20to%20use%20it%20as%20weights.%20The%20normalization%20should%20be%20done%20after%20the%20masking%20to%20prevent%20data%20leak%20from%20futures.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(masked_simple)%3A%0A%20%20%20%20row_sums%20%3D%20masked_simple.sum(dim%3D-1%2C%20keepdim%3DTrue)%0A%20%20%20%20masked_simple_norm%20%3D%20masked_simple%20%2F%20row_sums%0A%20%20%20%20print(masked_simple_norm)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20For%20more%20efficiency%2C%20this%20masking%20is%20replaced%20by%20filling%20%60-inf%60%20to%20upper%20right%20of%20attention%20scores%20before%20taking%20Softmax.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(attn_scores%2C%20context_length)%3A%0A%20%20%20%20mask%20%3D%20torch.triu(torch.ones(context_length%2C%20context_length)%2C%20diagonal%3D1)%0A%20%20%20%20masked%20%3D%20attn_scores.masked_fill(mask.bool()%2C%20-torch.inf)%0A%20%20%20%20print(masked)%0A%20%20%20%20return%20(masked%2C)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%24%5Cexp(-%5Cinfty)%3D0%24%20leads%20the%20same%20result%20with%20the%20masking.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(keys%2C%20masked)%3A%0A%20%20%20%20causal_attn_weights%20%3D%20torch.softmax(masked%20%2F%20keys.shape%5B-1%5D%20**%200.5%2C%20dim%3D-1)%0A%20%20%20%20print(causal_attn_weights)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%203.5.2%20Masking%20additional%20attention%20weights%20with%20dropout%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20We%20introduct%20dropout%20to%20prevent%20overfitting%20to%20specific%20tokens.%20We%20drop%2050%25%20of%20elements%20and%20the%20remaining%20values%20are%20doubled%20to%20keep%20the%20total%20scale.%20This%20dropout%20layer%20is%20applied%20only%20for%20training.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20torch.manual_seed(123)%0A%20%20%20%20dropout%20%3D%20torch.nn.Dropout(0.5)%20%20%23%20dropout%20rate%20of%2050%25%0A%20%20%20%20example%20%3D%20torch.ones(6%2C%206)%20%20%23%20create%20a%20matrix%20of%20ones%0A%0A%20%20%20%20print(dropout(example))%0A%20%20%20%20return%20(dropout%2C)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20The%20dropout%20layer%20is%20applied%20after%20attention%20weights%20are%20calculated.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(attn_weights%2C%20dropout)%3A%0A%20%20%20%20torch.manual_seed(123)%0A%20%20%20%20print(dropout(attn_weights))%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%203.5.3%20Implementing%20a%20compact%20causal%20self-attention%20class%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20We%20introduce%20such%20causal%20attention%2C%20dropout%2C%20and%20batch%20inference%20to%20attention%20layer.%20The%20batch%20input%20for%20testing%20is%20as%20follows.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(inputs)%3A%0A%20%20%20%20batch%20%3D%20torch.stack((inputs%2C%20inputs)%2C%20dim%3D0)%0A%20%20%20%20%23%202%20inputs%20with%206%20tokens%20each%2C%20and%20each%20token%20has%20embedding%20dimension%203%0A%20%20%20%20print(batch.shape)%20%20%0A%20%20%20%20return%20(batch%2C)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20This%20is%20an%20example%20for%20causal%20attention%20layer.%20%5B%60register_buffer%60%5D(https%3A%2F%2Fdocs.pytorch.org%2Fdocs%2Fstable%2Fgenerated%2Ftorch.nn.Module.html%23torch.nn.Module.register_buffer)%20is%20used%20to%20define%20constant%20(non-trained)%20parameters%20and%20ensure%20it%20is%20allocated%20on%20suitable%20device.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.class_definition%0Aclass%20CausalAttention(nn.Module)%3A%0A%20%20%20%20def%20__init__(self%2C%20d_in%2C%20d_out%2C%20context_length%2C%20dropout%2C%20qkv_bias%3DFalse)%3A%0A%20%20%20%20%20%20%20%20super().__init__()%0A%20%20%20%20%20%20%20%20self.d_out%20%3D%20d_out%0A%20%20%20%20%20%20%20%20self.W_query%20%3D%20nn.Linear(d_in%2C%20d_out%2C%20bias%3Dqkv_bias)%0A%20%20%20%20%20%20%20%20self.W_key%20%3D%20nn.Linear(d_in%2C%20d_out%2C%20bias%3Dqkv_bias)%0A%20%20%20%20%20%20%20%20self.W_value%20%3D%20nn.Linear(d_in%2C%20d_out%2C%20bias%3Dqkv_bias)%0A%0A%20%20%20%20%20%20%20%20%23%20NEW%0A%20%20%20%20%20%20%20%20self.dropout%20%3D%20nn.Dropout(dropout)%0A%20%20%20%20%20%20%20%20%23%20NEW%3A%20allocate%20maximum%20mask%20to%20be%20sliced%20later%0A%20%20%20%20%20%20%20%20self.register_buffer(%0A%20%20%20%20%20%20%20%20%20%20%20%20%22mask%22%2C%20torch.triu(torch.ones(context_length%2C%20context_length)%2C%20diagonal%3D1)%0A%20%20%20%20%20%20%20%20)%0A%0A%20%20%20%20def%20forward(self%2C%20x)%3A%0A%20%20%20%20%20%20%20%20b%2C%20num_tokens%2C%20d_in%20%3D%20x.shape%20%20%23%20New%20batch%20dimension%20b%0A%0A%20%20%20%20%20%20%20%20%23%20For%20inputs%20where%20%60num_tokens%60%20exceeds%20%60context_length%60%2C%20this%20will%20result%20in%20errors%0A%20%20%20%20%20%20%20%20%23%20in%20the%20mask%20creation%20further%20below.%0A%20%20%20%20%20%20%20%20%23%20In%20practice%2C%20this%20is%20not%20a%20problem%20since%20the%20LLM%20(chapters%204-7)%20ensures%20that%20inputs%0A%20%20%20%20%20%20%20%20%23%20do%20not%20exceed%20%60context_length%60%20before%20reaching%20this%20forward%20method.%0A%20%20%20%20%20%20%20%20keys%20%3D%20self.W_key(x)%20%20%20%20%23%20(B%2CN%2Cd_out)%0A%20%20%20%20%20%20%20%20queries%20%3D%20self.W_query(x)%20%20%20%23%20(B%2CN%2Cd_out)%0A%20%20%20%20%20%20%20%20values%20%3D%20self.W_value(x)%20%20%20%20%23%20(B%2CN%2Cd_out)%0A%0A%20%20%20%20%20%20%20%20%23%20Changed%20transpose%3A%20(B%2CN%2Cd_out)%20-%3E%20(B%2Cd_out%2CN)%0A%20%20%20%20%20%20%20%20attn_scores%20%3D%20queries%20%40%20keys.transpose(1%2C%202)%20%20%20%20%23%20(B%2CN%2CN)%0A%20%20%20%20%20%20%20%20%23%20New%2C%20_%20ops%20are%20in-place%0A%20%20%20%20%20%20%20%20%23%20%60%3Anum_tokens%60%20to%20account%20for%20cases%20where%20the%20number%20of%20tokens%20in%20the%20batch%20is%20smaller%20than%20the%20supported%20context_size%0A%20%20%20%20%20%20%20%20attn_scores.masked_fill_(%20%20%0A%20%20%20%20%20%20%20%20%20%20%20%20self.mask.bool()%5B%3Anum_tokens%2C%20%3Anum_tokens%5D%2C%20-torch.inf%0A%20%20%20%20%20%20%20%20)%20%20%0A%20%20%20%20%20%20%20%20%23%20normalization%0A%20%20%20%20%20%20%20%20attn_weights%20%3D%20torch.softmax(attn_scores%20%2F%20keys.shape%5B-1%5D%20**%200.5%2C%20dim%3D-1)%0A%20%20%20%20%20%20%20%20attn_weights%20%3D%20self.dropout(attn_weights)%20%20%23%20New%0A%0A%20%20%20%20%20%20%20%20context_vec%20%3D%20attn_weights%20%40%20values%20%23%20(B%2CN%2Cd_out)%0A%20%20%20%20%20%20%20%20return%20context_vec%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20This%20is%20an%20example%20to%20use%20the%20forward%20path.%20Check%20the%20input%20and%20output%20shapes.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(batch%2C%20d_in%2C%20d_out)%3A%0A%20%20%20%20torch.manual_seed(123)%0A%0A%20%20%20%20%23%20%60batch%60%20has%20(B%2CN%2CC)%20shape%0A%20%20%20%20_context_length%20%3D%20batch.shape%5B1%5D%0A%20%20%20%20ca%20%3D%20CausalAttention(d_in%2C%20d_out%2C%20_context_length%2C%200.0)%0A%0A%20%20%20%20_context_vecs%20%3D%20ca(batch)%0A%0A%20%20%20%20print(_context_vecs)%0A%0A%20%20%20%20print(f%22%7Bbatch.shape%3D%7D%22)%0A%20%20%20%20print(f%22%7B_context_vecs.shape%3D%7D%22)%0A%20%20%20%20print(f%22%7Bd_in%3D%7D%2C%20%7Bd_out%3D%7D%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%203.6%20Extending%20single-head%20attention%20to%20multi-head%20attention%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%203.6.1%20Stacking%20multiple%20single-head%20attention%20layers%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20Let's%20extend%20the%20attention%20layer%20to%20multi-head.%20This%20is%20a%20naive%20extetion%20from%20single-head%20one.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.class_definition%0Aclass%20MultiHeadAttentionWrapper(nn.Module)%3A%0A%20%20%20%20def%20__init__(self%2C%20d_in%2C%20d_out%2C%20context_length%2C%20dropout%2C%20num_heads%2C%20qkv_bias%3DFalse)%3A%0A%20%20%20%20%20%20%20%20super().__init__()%0A%20%20%20%20%20%20%20%20self.heads%20%3D%20nn.ModuleList(%0A%20%20%20%20%20%20%20%20%20%20%20%20%5B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20CausalAttention(d_in%2C%20d_out%2C%20context_length%2C%20dropout%2C%20qkv_bias)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20for%20_%20in%20range(num_heads)%0A%20%20%20%20%20%20%20%20%20%20%20%20%5D%0A%20%20%20%20%20%20%20%20)%0A%0A%20%20%20%20def%20forward(self%2C%20x)%3A%0A%20%20%20%20%20%20%20%20%23%20just%20apply%20single-head%20attentions%20and%20concatenate%20their%20outputs%0A%20%20%20%20%20%20%20%20return%20torch.cat(%5Bhead(x)%20for%20head%20in%20self.heads%5D%2C%20dim%3D-1)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20The%20output%20shape%20is%20%24(B%2CN%2Cd_%7Bout%7D%5Ctimes2)%24.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(batch)%3A%0A%20%20%20%20torch.manual_seed(123)%0A%0A%20%20%20%20_context_length%20%3D%20batch.shape%5B1%5D%20%20%23%20This%20is%20the%20number%20of%20tokens%0A%20%20%20%20_d_in%2C%20_d_out%20%3D%203%2C%202%0A%20%20%20%20_mha%20%3D%20MultiHeadAttentionWrapper(_d_in%2C%20_d_out%2C%20_context_length%2C%200.0%2C%20num_heads%3D2)%0A%0A%20%20%20%20_context_vecs%20%3D%20_mha(batch)%0A%20%20%20%20print(_context_vecs)%0A%20%20%20%20print(f%22%7Bbatch.shape%3D%7D%22)%0A%20%20%20%20print(f%22%7B_context_vecs.shape%3D%7D%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20%23%23%23%203.6.2%20Implementing%20multi-head%20attention%20with%20weight%20splits%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20mo.md(r%22%22%22%0A%20%20%20%20Simplify%20the%20multi-head%20attention%20layer%20by%20self-contained%20way%20and%20avoid%20loops%20by%20using%20matrix%20products.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.class_definition%0Aclass%20MultiHeadAttention(nn.Module)%3A%0A%20%20%20%20def%20__init__(self%2C%20d_in%2C%20d_out%2C%20context_length%2C%20dropout%2C%20num_heads%2C%20qkv_bias%3DFalse)%3A%0A%20%20%20%20%20%20%20%20super().__init__()%0A%20%20%20%20%20%20%20%20assert%20d_out%20%25%20num_heads%20%3D%3D%200%2C%20%22d_out%20must%20be%20divisible%20by%20num_heads%22%0A%0A%20%20%20%20%20%20%20%20self.d_out%20%3D%20d_out%0A%20%20%20%20%20%20%20%20self.num_heads%20%3D%20num_heads%0A%20%20%20%20%20%20%20%20self.head_dim%20%3D%20(%0A%20%20%20%20%20%20%20%20%20%20%20%20d_out%20%2F%2F%20num_heads%0A%20%20%20%20%20%20%20%20)%20%20%23%20Reduce%20the%20projection%20dim%20to%20match%20desired%20output%20dim%0A%0A%20%20%20%20%20%20%20%20self.W_query%20%3D%20nn.Linear(d_in%2C%20d_out%2C%20bias%3Dqkv_bias)%0A%20%20%20%20%20%20%20%20self.W_key%20%3D%20nn.Linear(d_in%2C%20d_out%2C%20bias%3Dqkv_bias)%0A%20%20%20%20%20%20%20%20self.W_value%20%3D%20nn.Linear(d_in%2C%20d_out%2C%20bias%3Dqkv_bias)%0A%20%20%20%20%20%20%20%20self.out_proj%20%3D%20nn.Linear(d_out%2C%20d_out)%20%20%23%20Linear%20layer%20to%20combine%20head%20outputs%0A%20%20%20%20%20%20%20%20self.dropout%20%3D%20nn.Dropout(dropout)%0A%20%20%20%20%20%20%20%20%23%20constant%20parameters%0A%20%20%20%20%20%20%20%20self.register_buffer(%0A%20%20%20%20%20%20%20%20%20%20%20%20%22mask%22%2C%20torch.triu(torch.ones(context_length%2C%20context_length)%2C%20diagonal%3D1)%0A%20%20%20%20%20%20%20%20)%0A%0A%20%20%20%20def%20forward(self%2C%20x)%3A%0A%20%20%20%20%20%20%20%20b%2C%20num_tokens%2C%20d_in%20%3D%20x.shape%0A%20%20%20%20%20%20%20%20%23%20As%20in%20%60CausalAttention%60%2C%20for%20inputs%20where%20%60num_tokens%60%20exceeds%20%60context_length%60%2C%0A%20%20%20%20%20%20%20%20%23%20this%20will%20result%20in%20errors%20in%20the%20mask%20creation%20further%20below.%0A%20%20%20%20%20%20%20%20%23%20In%20practice%2C%20this%20is%20not%20a%20problem%20since%20the%20LLM%20(chapters%204-7)%20ensures%20that%20inputs%0A%20%20%20%20%20%20%20%20%23%20do%20not%20exceed%20%60context_length%60%20before%20reaching%20this%20forward%20method.%0A%0A%20%20%20%20%20%20%20%20keys%20%3D%20self.W_key(x)%20%20%23%20Shape%3A%20(b%2C%20num_tokens%2C%20d_out)%0A%20%20%20%20%20%20%20%20queries%20%3D%20self.W_query(x)%0A%20%20%20%20%20%20%20%20values%20%3D%20self.W_value(x)%0A%0A%20%20%20%20%20%20%20%20%23%20We%20implicitly%20split%20the%20matrix%20by%20adding%20a%20%60num_heads%60%20dimension%0A%20%20%20%20%20%20%20%20%23%20Unroll%20last%20dim%3A%20(b%2C%20num_tokens%2C%20d_out)%20-%3E%20(b%2C%20num_tokens%2C%20num_heads%2C%20head_dim)%0A%20%20%20%20%20%20%20%20%23%20So%2C%20d_out%20%3D%20num_heads%20*%20head_dim%20(see%20this%20constructor)%0A%20%20%20%20%20%20%20%20keys%20%3D%20keys.view(b%2C%20num_tokens%2C%20self.num_heads%2C%20self.head_dim)%0A%20%20%20%20%20%20%20%20values%20%3D%20values.view(b%2C%20num_tokens%2C%20self.num_heads%2C%20self.head_dim)%0A%20%20%20%20%20%20%20%20queries%20%3D%20queries.view(b%2C%20num_tokens%2C%20self.num_heads%2C%20self.head_dim)%0A%0A%20%20%20%20%20%20%20%20%23%20Transpose%3A%20(b%2C%20num_tokens%2C%20num_heads%2C%20head_dim)%20-%3E%20(b%2C%20num_heads%2C%20num_tokens%2C%20head_dim)%0A%20%20%20%20%20%20%20%20keys%20%3D%20keys.transpose(1%2C%202)%0A%20%20%20%20%20%20%20%20queries%20%3D%20queries.transpose(1%2C%202)%0A%20%20%20%20%20%20%20%20values%20%3D%20values.transpose(1%2C%202)%0A%0A%20%20%20%20%20%20%20%20%23%20Compute%20scaled%20dot-product%20attention%20(aka%20self-attention)%20with%20a%20causal%20mask%0A%20%20%20%20%20%20%20%20%23%20Shape%3A%20(b%2C%20num_heads%2C%20num_tokens%2C%20num_tokens)%0A%20%20%20%20%20%20%20%20attn_scores%20%3D%20queries%20%40%20keys.transpose(2%2C%203)%20%20%23%20Dot%20product%20for%20each%20head%0A%0A%20%20%20%20%20%20%20%20%23%20Original%20mask%20truncated%20to%20the%20number%20of%20tokens%20and%20converted%20to%20boolean%0A%20%20%20%20%20%20%20%20mask_bool%20%3D%20self.mask.bool()%5B%3Anum_tokens%2C%20%3Anum_tokens%5D%0A%0A%20%20%20%20%20%20%20%20%23%20Use%20the%20mask%20to%20fill%20attention%20scores%0A%20%20%20%20%20%20%20%20attn_scores.masked_fill_(mask_bool%2C%20-torch.inf)%0A%0A%20%20%20%20%20%20%20%20attn_weights%20%3D%20torch.softmax(attn_scores%20%2F%20keys.shape%5B-1%5D%20**%200.5%2C%20dim%3D-1)%0A%20%20%20%20%20%20%20%20attn_weights%20%3D%20self.dropout(attn_weights)%0A%0A%20%20%20%20%20%20%20%20%23%20Shape%3A%20(b%2C%20num_tokens%2C%20num_heads%2C%20head_dim)%0A%20%20%20%20%20%20%20%20context_vec%20%3D%20(attn_weights%20%40%20values).transpose(1%2C%202)%0A%0A%20%20%20%20%20%20%20%20%23%20Combine%20heads%2C%20where%20self.d_out%20%3D%20self.num_heads%20*%20self.head_dim%0A%20%20%20%20%20%20%20%20context_vec%20%3D%20context_vec.contiguous().view(b%2C%20num_tokens%2C%20self.d_out)%0A%20%20%20%20%20%20%20%20context_vec%20%3D%20self.out_proj(context_vec)%20%20%23%20optional%20projection%0A%0A%20%20%20%20%20%20%20%20return%20context_vec%0A%0A%0A%40app.cell%0Adef%20_(batch)%3A%0A%20%20%20%20torch.manual_seed(123)%0A%0A%20%20%20%20batch_size%2C%20context_length%2C%20_d_in%20%3D%20batch.shape%0A%20%20%20%20_d_out%20%3D%202%0A%20%20%20%20mha%20%3D%20MultiHeadAttention(_d_in%2C%20_d_out%2C%20context_length%2C%200.0%2C%20num_heads%3D2)%0A%0A%20%20%20%20context_vecs%20%3D%20mha(batch)%0A%0A%20%20%20%20print(context_vecs)%0A%20%20%20%20print(%22context_vecs.shape%3A%22%2C%20context_vecs.shape)%0A%20%20%20%20return%20(context_length%2C)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20%23%20(b%2C%20num_heads%2C%20num_tokens%2C%20head_dim)%20%3D%20(1%2C%202%2C%203%2C%204)%0A%20%20%20%20a%20%3D%20torch.tensor(%0A%20%20%20%20%20%20%20%20%5B%0A%20%20%20%20%20%20%20%20%20%20%20%20%5B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5B0.2745%2C%200.6584%2C%200.2775%2C%200.8573%5D%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5B0.8993%2C%200.0390%2C%200.9268%2C%200.7388%5D%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5B0.7179%2C%200.7058%2C%200.9156%2C%200.4340%5D%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5D%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5B%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5B0.0772%2C%200.3565%2C%200.1479%2C%200.5331%5D%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5B0.4066%2C%200.2318%2C%200.4545%2C%200.9737%5D%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5B0.4606%2C%200.5159%2C%200.4220%2C%200.5786%5D%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5D%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%5D%0A%20%20%20%20%20%20%20%20%5D%0A%20%20%20%20)%0A%0A%20%20%20%20print(a%20%40%20a.transpose(2%2C%203))%0A%20%20%20%20return%20(a%2C)%0A%0A%0A%40app.cell%0Adef%20_(a)%3A%0A%20%20%20%20_first_head%20%3D%20a%5B0%2C%200%2C%20%3A%2C%20%3A%5D%0A%20%20%20%20first_res%20%3D%20_first_head%20%40%20_first_head.T%0A%20%20%20%20print(%22First%20head%3A%5Cn%22%2C%20first_res)%0A%0A%20%20%20%20_second_head%20%3D%20a%5B0%2C%201%2C%20%3A%2C%20%3A%5D%0A%20%20%20%20_second_res%20%3D%20_second_head%20%40%20_second_head.T%0A%20%20%20%20print(%22%5CnSecond%20head%3A%5Cn%22%2C%20_second_res)%0A%20%20%20%20return%0A%0A%0Aif%20__name__%20%3D%3D%20%22__main__%22%3A%0A%20%20%20%20app.run()%0A
</marimo-code>

<marimo-code-hash hidden="">eed7a2fa50d689556c85e34a695f8787</marimo-code-hash>
</body>
</html>
